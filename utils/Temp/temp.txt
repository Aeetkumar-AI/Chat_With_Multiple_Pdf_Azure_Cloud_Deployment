import json
from langchain.schema import Document, SystemMessage, HumanMessage
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate

# =====================================================
# COMPLETE SYSTEM PROMPT WITH METADATA
# =====================================================

CHUNKING_SYSTEM_PROMPT = """
You are an expert document-chunking assistant specializing in PCM (Process Control Manual), compliance documents, procedural manuals, and technical documentation. Your task is to intelligently segment long documents into semantically meaningful chunks while preserving structural integrity, formatting, and logical relationships.

=== CORE OBJECTIVE ===
Split documents into chunks that:
1. Maintain complete semantic meaning (no orphaned content)
2. Preserve all formatting, tables, lists, and metadata
3. Respect natural document boundaries (sections, procedures, workflows)
4. Optimize for vector database ingestion (FAISS, Chroma, Pinecone, Postgres pgvector)
5. Enable accurate retrieval for RAG (Retrieval-Augmented Generation) systems

=== STRICT CHUNKING RULES ===

**Rule 1: NEVER split these elements**
- Document metadata blocks (Version, Owner, SOEID, Application Name, PCM ID, etc.)
- Complete Table of Contents (entire TOC stays in one chunk)
- Any table (regardless of size or format)
- Complete bullet or numbered lists
- Step-by-step procedures (keep all steps together unless >800 words with clear subsections)
- Workflow diagrams (Ticket Flow, Event Flow, Escalation Matrix)
- Revision History tables
- Headings and their immediately following content (H1-H6 must stay with their section)
- Images with their captions or references
- Code blocks or technical snippets
- UNC paths, URLs, and hyperlinks with their context

**Rule 2: Preserve all original formatting**
- Maintain bold (**text**), italics (*text*), underlines
- Keep markdown formatting intact
- Preserve indentation and spacing
- Retain special characters and symbols
- Keep line breaks within structured content (tables, code blocks)

**Rule 3: Semantic boundaries take priority**
- A chunk must represent ONE complete idea, section, or procedure
- Never end a chunk mid-sentence, mid-paragraph, or mid-table
- Never separate a heading from its content
- Never split closely related paragraphs that form a single concept

**Rule 4: Chunk size guidelines**
- **Target**: 300‚Äì600 words per chunk
- **Minimum**: 100 words (except for standalone metadata or short TOC)
- **Maximum**: Flexible‚ÄîALWAYS prioritize semantic completeness over word count
- If a table, procedure, or section is 1,200 words but cannot be split logically, keep it as ONE chunk
- For lists or procedures exceeding 800 words with clear subsections, you MAY split at natural subsection boundaries

**Rule 5: Handle nested structures carefully**
- Tables containing bullet points ‚Üí keep entire table together
- Lists containing sub-tables ‚Üí keep entire list together
- Procedures with embedded diagrams ‚Üí keep together
- Treat the outermost container as the atomic unit

**Rule 6: Cross-references and links**
- Preserve all cross-references as-is (e.g., "See Section 3.2")
- Do NOT merge distant sections just because they reference each other
- Keep hyperlinks with their surrounding context
- Keep UNC paths with their descriptive text

**Rule 7: Images and diagrams**
- If an image is referenced in text, include it in the same chunk
- Keep image placeholders with their context
- If an image appears between paragraphs without explicit reference, include it with the preceding paragraph

=== METADATA EXTRACTION REQUIREMENTS ===

For each chunk, you MUST extract and include metadata. This is critical for retrieval accuracy.

**Document-Level Metadata** (extract once from document header - if available):
- document_id: PCM ID or unique document identifier
- document_title: Full document title
- version: Document version number
- document_owner: Owner name
- owner_soeid: Owner's SOEID
- document_approver: Approver name (if present)
- approver_soeid: Approver's SOEID (if present)
- application_name: Application name
- csi_app_ids: Array of CSI APP IDs (split by semicolon or comma)
- cisar_ids: Array of CISAR IDs
- isa_contact_id: ISA Contact ID
- annual_review_date: Annual review date (YYYY-MM-DD format)
- last_updated: Last update date
- document_type: Type (PCM, Procedure Manual, Compliance Guide, etc.)

**Chunk-Level Metadata** (for EACH chunk):

**Required Fields:**
- section_title: The main heading/title of this chunk
- section_number: Section number (e.g., "4.1", "TOC", "0" for header)
- heading_hierarchy: Array of parent headings from top to bottom
- word_count: Approximate word count
- chunk_type: Classification (see below)

**Chunk Type Classification:**
Use ONE of these values:
- "metadata_block" - Document header with version, owner, IDs
- "table_of_contents" - Complete TOC
- "revision_history" - Revision/change history table
- "section_content" - Regular prose/paragraph content
- "procedure" - Step-by-step procedures
- "workflow_table" - Workflow, escalation, or status tables
- "data_table" - Data tables (access lists, user tables)
- "sampling_methodology" - Sampling or testing procedures
- "event_counting" - Event counting methodologies
- "control_description" - Control descriptions and objectives
- "appendix" - Appendix or reference material

**Optional But Recommended Fields:**
- page_number: Page number (estimate if not marked)
- contains_tables: Boolean - does chunk contain tables?
- contains_procedures: Boolean - does chunk contain procedures?
- contains_images: Boolean - does chunk contain images?
- table_names: Array of table titles in this chunk
- procedure_step_count: Number of steps if procedure
- keywords: Array of 3-7 key terms from chunk
- related_sections: Array of section numbers referenced
- unc_paths: Array of UNC paths mentioned
- urls: Array of URLs mentioned

=== OUTPUT FORMAT ===

Return ONLY valid JSON in this EXACT structure:

{
  "document_metadata": {
    "document_id": "string or null",
    "document_title": "string or null",
    "version": "string or null",
    "document_owner": "string or null",
    "owner_soeid": "string or null",
    "document_approver": "string or null",
    "approver_soeid": "string or null",
    "application_name": "string or null",
    "csi_app_ids": ["string"] or [],
    "cisar_ids": ["string"] or [],
    "isa_contact_id": "string or null",
    "annual_review_date": "string or null",
    "last_updated": "string or null",
    "document_type": "string or null",
    "total_chunks": number
  },
  "chunks": [
    {
      "chunk_id": number,
      "content": "escaped string content",
      "metadata": {
        "section_title": "string",
        "section_number": "string",
        "heading_hierarchy": ["string"],
        "word_count": number,
        "chunk_type": "string",
        "contains_tables": boolean,
        "contains_procedures": boolean,
        "contains_images": boolean,
        "keywords": ["string"],
        "page_number": number or null,
        "table_names": ["string"] or null,
        "procedure_step_count": number or null,
        "related_sections": ["string"] or null,
        "unc_paths": ["string"] or null,
        "urls": ["string"] or null
      }
    }
  ]
}

**Critical JSON Requirements:**
- Escape all special characters: \\n for newlines, \\" for quotes, \\\\ for backslashes
- All string values must be in double quotes
- Boolean values: true/false (lowercase, no quotes)
- Null values: null (lowercase, no quotes)
- Arrays can be empty [] but must exist for required fields
- Numbers have no quotes

=== QUALITY CONTROL CHECKLIST ===

Before returning, verify:
‚úì Each chunk is semantically complete
‚úì No mid-sentence or mid-table breaks
‚úì All metadata fields are populated
‚úì Headings are with their content
‚úì Tables are complete
‚úì Valid JSON syntax
‚úì All special characters are escaped

=== FINAL INSTRUCTION ===

You will receive a document to chunk. Apply ALL rules meticulously. Return ONLY the JSON structure with no additional text, explanation, or markdown code blocks. The response must start with { and end with }.
"""

# =====================================================
# CHUNKING FUNCTION WITH METADATA EXTRACTION
# =====================================================

def agentic_chunking_with_metadata(text, llm, source_file):
    """
    Dynamically splits text into meaningful chunks with rich metadata using LLM.
    
    Args:
        text: Document text to chunk
        llm: LangChain LLM instance
        source_file: Path to source file
        
    Returns:
        List of Document objects with content and metadata
    """
    
    system_message = SystemMessage(content=CHUNKING_SYSTEM_PROMPT)
    
    human_message = HumanMessage(content=f"""
Document to chunk:

{text}
""")
    
    # Get LLM response
    response = llm.invoke([system_message, human_message])
    
    # Extract content from response
    if hasattr(response, 'content'):
        response_text = response.content
    else:
        response_text = str(response)
    
    # Clean response (remove markdown code blocks if present)
    response_text = response_text.strip()
    if response_text.startswith('```json'):
        response_text = response_text[7:]
    if response_text.startswith('```'):
        response_text = response_text[3:]
    if response_text.endswith('```'):
        response_text = response_text[:-3]
    response_text = response_text.strip()
    
    try:
        # Parse JSON response
        parsed_response = json.loads(response_text)
        
        # Extract document-level metadata
        doc_metadata = parsed_response.get("document_metadata", {})
        chunks_data = parsed_response.get("chunks", [])
        
        # Create Document objects with combined metadata
        documents = []
        for chunk_data in chunks_data:
            # Combine document-level and chunk-level metadata
            combined_metadata = {
                # Source information
                "source": source_file,
                "source_type": "file",
                
                # Document-level metadata
                "document_id": doc_metadata.get("document_id"),
                "document_title": doc_metadata.get("document_title"),
                "version": doc_metadata.get("version"),
                "document_owner": doc_metadata.get("document_owner"),
                "owner_soeid": doc_metadata.get("owner_soeid"),
                "application_name": doc_metadata.get("application_name"),
                "document_type": doc_metadata.get("document_type"),
                "annual_review_date": doc_metadata.get("annual_review_date"),
                
                # Chunk-level metadata
                "chunk_id": chunk_data.get("chunk_id"),
                "section_title": chunk_data["metadata"].get("section_title"),
                "section_number": chunk_data["metadata"].get("section_number"),
                "heading_hierarchy": chunk_data["metadata"].get("heading_hierarchy", []),
                "word_count": chunk_data["metadata"].get("word_count"),
                "chunk_type": chunk_data["metadata"].get("chunk_type"),
                "contains_tables": chunk_data["metadata"].get("contains_tables", False),
                "contains_procedures": chunk_data["metadata"].get("contains_procedures", False),
                "contains_images": chunk_data["metadata"].get("contains_images", False),
                "keywords": chunk_data["metadata"].get("keywords", []),
                "page_number": chunk_data["metadata"].get("page_number"),
                "table_names": chunk_data["metadata"].get("table_names"),
                "procedure_step_count": chunk_data["metadata"].get("procedure_step_count"),
                "related_sections": chunk_data["metadata"].get("related_sections"),
                "unc_paths": chunk_data["metadata"].get("unc_paths"),
                "urls": chunk_data["metadata"].get("urls"),
            }
            
            # Create Document object
            doc = Document(
                page_content=chunk_data["content"],
                metadata=combined_metadata
            )
            documents.append(doc)
        
        print(f"‚úÖ Successfully chunked into {len(documents)} chunks with metadata")
        return documents
        
    except json.JSONDecodeError as e:
        print(f"‚ùå Error parsing JSON response: {e}")
        print(f"Response preview: {response_text[:500]}")
        
        # Fallback: Basic chunking without metadata
        print("‚ö†Ô∏è Falling back to basic chunking...")
        chunks = response_text.split("\n\n")
        documents = []
        for i, chunk in enumerate(chunks):
            if chunk.strip():
                doc = Document(
                    page_content=chunk.strip(),
                    metadata={
                        "source": source_file,
                        "chunk_id": i + 1,
                        "chunk_type": "fallback",
                        "section_title": f"Chunk {i + 1}",
                        "section_number": str(i + 1)
                    }
                )
                documents.append(doc)
        return documents

# =====================================================
# COMPLETE IMPLEMENTATION
# =====================================================

# Read document
file_path = "text.md"
with open(file_path, 'r', encoding='utf-8') as file:
    file_content = file.read()

# Initialize your LLM (example with ChatOpenAI)
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=0)

# Or with Claude
# from langchain_anthropic import ChatAnthropic
# llm = ChatAnthropic(model="claude-sonnet-4-20250514", temperature=0)

# Chunk with metadata
chunks_docs = agentic_chunking_with_metadata(
    text=file_content,
    llm=llm,
    source_file=file_path
)

print(f"\nüìä Chunking Results:")
print(f"Total chunks: {len(chunks_docs)}")
print(f"\nFirst chunk metadata:")
print(json.dumps(chunks_docs[0].metadata, indent=2))

# =====================================================
# CREATE VECTOR STORE WITH METADATA
# =====================================================

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# Create FAISS vector store with metadata
vectorstore = FAISS.from_documents(chunks_docs, embeddings)

# Create retriever with MMR
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 7}
)

# =====================================================
# ENHANCED PROMPT WITH METADATA AWARENESS
# =====================================================

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an expert GCM (Global Compliance Monitoring) and PCM (Process Controls Manual) analyst with comprehensive knowledge of GIAM Operations, security report reconciliation, escalation procedures, and compliance monitoring processes.

You have access to context from PCM documents with rich metadata including:
- Document versions, owners, and identifiers
- Section numbers and hierarchies
- Content types (procedures, tables, workflows)
- Keywords and related sections

Enhanced Context from GCM/PCM Knowledge Base:
{context}

Instructions for Expert Analysis:
1. Provide comprehensive answers synthesizing all relevant information
2. Include specific step-by-step procedures when applicable
3. Extract and present table data with clear formatting
4. Reference specific PCM sections using the metadata (e.g., "According to Section 4.1 of PCM000000 v3.1...")
5. When citing procedures, mention the procedure step count if available
6. Reference document owners and version information when relevant
7. Use clear, concise language with no unnecessary jargon
8. Ensure all answers are actionable and directly address the question asked
9. If information comes from tables, workflows, or specific chunk types, mention this for credibility

Answer in a clear, actionable format that directly addresses the user's question:"""),
    ("human", "Question: {question}")
])

# =====================================================
# ENHANCED RUN FUNCTION WITH METADATA FILTERING
# =====================================================

def run(query, filter_chunk_type=None, filter_section=None):
    """
    Run query with optional metadata filtering
    
    Args:
        query: User question
        filter_chunk_type: Optional filter for chunk_type (e.g., "procedure", "workflow_table")
        filter_section: Optional filter for section_number (e.g., "4.1")
    """
    # Get relevant documents
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 10}
    )
    docs = retriever.invoke(query)
    
    # Optional: Apply metadata filters
    if filter_chunk_type:
        docs = [doc for doc in docs if doc.metadata.get("chunk_type") == filter_chunk_type]
    
    if filter_section:
        docs = [doc for doc in docs if doc.metadata.get("section_number") == filter_section]
    
    # Build enhanced context with metadata
    context_parts = []
    for doc in docs:
        meta = doc.metadata
        context_header = f"""
[Source: {meta.get('document_title', 'Unknown')} v{meta.get('version', 'N/A')}]
[Section: {meta.get('section_number', 'N/A')} - {meta.get('section_title', 'N/A')}]
[Type: {meta.get('chunk_type', 'N/A')}]
[Document ID: {meta.get('document_id', 'N/A')}]
"""
        context_parts.append(context_header + "\n" + doc.page_content)
    
    context = "\n\n---\n\n".join(context_parts)
    
    # Run chain
    chain = prompt | llm
    inputs = {
        "context": context,
        "question": query
    }
    
    response = chain.invoke(inputs)
    
    # Print source metadata
    print("\nüìö Sources Used:")
    for i, doc in enumerate(docs[:5], 1):
        meta = doc.metadata
        print(f"{i}. {meta.get('section_title')} (Section {meta.get('section_number')}) - {meta.get('chunk_type')}")
    
    return response

# =====================================================
# USAGE EXAMPLES
# =====================================================

# Example 1: General query
result = run("What is the access request submission process?")
print(result.content if hasattr(result, 'content') else result)

# Example 2: Filter by chunk type (only procedures)
result = run(
    "Show me all procedures",
    filter_chunk_type="procedure"
)

# Example 3: Filter by section
result = run(
    "What are the details in section 4.1?",
    filter_section="4.1"
)

# =====================================================
# ADVANCED: METADATA-BASED SEARCH
# =====================================================

def search_by_metadata(chunk_type=None, contains_tables=None, section_prefix=None):
    """
    Search documents by metadata criteria
    
    Args:
        chunk_type: Filter by chunk type
        contains_tables: Filter by whether chunk contains tables
        section_prefix: Filter by section prefix (e.g., "4" for all section 4.x)
    """
    all_docs = vectorstore.similarity_search("", k=1000)  # Get all docs
    
    filtered_docs = all_docs
    
    if chunk_type:
        filtered_docs = [d for d in filtered_docs if d.metadata.get("chunk_type") == chunk_type]
    
    if contains_tables is not None:
        filtered_docs = [d for d in filtered_docs if d.metadata.get("contains_tables") == contains_tables]
    
    if section_prefix:
        filtered_docs = [d for d in filtered_docs if d.metadata.get("section_number", "").startswith(section_prefix)]
    
    return filtered_docs

# Example: Get all workflow tables
workflow_tables = search_by_metadata(chunk_type="workflow_table")
print(f"\nFound {len(workflow_tables)} workflow tables")

# Example: Get all procedures with tables in section 4
section4_procedures = search_by_metadata(
    chunk_type="procedure",
    contains_tables=True,
    section_prefix="4"
)
print(f"Found {len(section4_procedures)} procedures with tables in section 4")

