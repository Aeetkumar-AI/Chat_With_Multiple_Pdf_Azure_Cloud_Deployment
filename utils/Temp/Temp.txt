from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# Step 1: Split by markdown headers to preserve hierarchy
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on,
    strip_headers=False  # Keep headers in chunks for context
)

# Step 2: Further split large sections if needed
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Adjust based on your needs
    chunk_overlap=200,  # Important for maintaining context
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)

# Load your markdown file
with open("text.md", "r", encoding="utf-8") as f:
    markdown_text = f.read()

# Split by headers first
md_header_splits = markdown_splitter.split_text(markdown_text)

# Then split further if chunks are too large
final_chunks = []
for doc in md_header_splits:
    if len(doc.page_content) > 1000:
        sub_chunks = text_splitter.split_documents([doc])
        final_chunks.extend(sub_chunks)
    else:
        final_chunks.append(doc)




from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Initialize embeddings
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key="YOUR_API_KEY"
)

# Create FAISS vector store
vectorstore = FAISS.from_documents(
    documents=final_chunks,
    embedding=embeddings
)

# Configure retriever with higher k for better context
retriever = vectorstore.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance for diversity
    search_kwargs={
        "k": 5,  # Retrieve top 5 chunks
        "fetch_k": 20  # Consider 20 candidates
    }
)

# Custom prompt for accuracy
prompt_template = """You are an expert assistant for GCM (Global Compliance Management) procedures.
Use the following context to answer the question accurately. If you're not sure, say so.

Context: {context}

Question: {question}

Important: 
- Cite specific section headers when referencing procedures
- Maintain exact timeframes (business days) mentioned
- Preserve escalation levels and hierarchies
- Include all required participants/stakeholders

Answer:"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# Initialize Gemini 2.5 Flash
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key="YOUR_API_KEY",
    temperature=0,  # Set to 0 for maximum accuracy
    convert_system_message_to_human=True
)

# Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": PROMPT}
)

# Test query
query = "What are the escalation timeframes for ESC2 level?"

result = qa_chain.invoke({"query": query})

print("Answer:", result['result'])
print("\nSource Documents:")
for i, doc in enumerate(result['source_documents']):
    print(f"\n--- Source {i+1} ---")
    print(doc.page_content[:200])
    print(doc.metadata)











from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
import re

# ============================================================================
# STEP 1: MARKDOWN CHUNKING WITH HIERARCHY PRESERVATION
# ============================================================================

# Define headers to split on
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on,
    strip_headers=False  # Keep headers in chunks for context
)

# Recursive splitter for large sections
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Optimal for technical docs
    chunk_overlap=200,  # Ensures context continuity
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)

# Load your markdown file
with open("text.md", "r", encoding="utf-8") as f:
    markdown_text = f.read()

# Split by headers first
md_header_splits = markdown_splitter.split_text(markdown_text)

# Then split further if chunks are too large
final_chunks = []
for doc in md_header_splits:
    if len(doc.page_content) > 1000:
        sub_chunks = text_splitter.split_documents([doc])
        final_chunks.extend(sub_chunks)
    else:
        final_chunks.append(doc)

print(f"Total chunks created: {len(final_chunks)}")

# ============================================================================
# STEP 2: METADATA ENRICHMENT FOR BETTER RETRIEVAL
# ============================================================================

def extract_timeframes(text):
    """Extract business day timeframes from text"""
    timeframe_pattern = r'(\d+)\s*business\s*days?'
    matches = re.findall(timeframe_pattern, text, re.IGNORECASE)
    return [int(m) for m in matches] if matches else []

def extract_escalation_level(text):
    """Extract escalation levels (IC, ESC1-4)"""
    levels = []
    if re.search(r'\bIC\b', text):
        levels.append('IC')
    for i in range(1, 5):
        if re.search(rf'\bESC{i}\b', text):
            levels.append(f'ESC{i}')
    return levels

def extract_ticket_types(text):
    """Extract ticket types mentioned"""
    ticket_types = []
    patterns = ['INC', 'MIM', 'CHG', 'CMP', 'OST', 'SLTN', 'ResolveIT', 'ServiceNow']
    for ticket_type in patterns:
        if ticket_type in text:
            ticket_types.append(ticket_type)
    return ticket_types

def extract_participants(text):
    """Extract key participants/roles"""
    participants = []
    roles = [
        'GIAM Operations', 'GCM team', 'manager', 'Application Manager',
        'Business Owner', 'ISO', 'CISO', 'Command Center', 'maker', 'modifier'
    ]
    for role in roles:
        if role.lower() in text.lower():
            participants.append(role)
    return list(set(participants))

# Enrich all chunks with metadata
for i, chunk in enumerate(final_chunks):
    content = chunk.page_content
    
    # Basic categorization
    chunk.metadata['chunk_id'] = i
    chunk.metadata['chunk_length'] = len(content)
    
    # Section type identification
    if 'escalation' in content.lower():
        chunk.metadata['section_type'] = 'escalation'
    elif 'ticket' in content.lower():
        chunk.metadata['section_type'] = 'ticket_management'
    elif 'reconcil' in content.lower():
        chunk.metadata['section_type'] = 'reconciliation'
    elif 'procedure' in content.lower():
        chunk.metadata['section_type'] = 'procedure'
    else:
        chunk.metadata['section_type'] = 'general'
    
    # Content features
    chunk.metadata['contains_table'] = '|' in content and '---' in content
    chunk.metadata['contains_file_path'] = 'file:///' in content or '\\\\' in content
    chunk.metadata['contains_link'] = '[' in content and '](' in content
    chunk.metadata['contains_code'] = '```' in content or '`' in content
    
    # Extract specific information
    chunk.metadata['timeframes'] = extract_timeframes(content)
    chunk.metadata['escalation_levels'] = extract_escalation_level(content)
    chunk.metadata['ticket_types'] = extract_ticket_types(content)
    chunk.metadata['participants'] = extract_participants(content)
    
    # Priority indicators
    chunk.metadata['has_deadline'] = bool(chunk.metadata['timeframes'])
    chunk.metadata['has_approval_process'] = 'approval' in content.lower()
    
    # Count key elements
    chunk.metadata['bullet_points'] = content.count('* ') + content.count('- ')
    chunk.metadata['numbered_steps'] = len(re.findall(r'^\d+\.', content, re.MULTILINE))

print("Metadata enrichment completed!")

# Display sample enriched metadata
if final_chunks:
    print("\nSample enriched chunk metadata:")
    print(f"Chunk 0 metadata: {final_chunks[0].metadata}")

# ============================================================================
# STEP 3: INITIALIZE EMBEDDINGS AND VECTOR STORE
# ============================================================================

# Initialize Google Generative AI embeddings
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key="YOUR_API_KEY"  # Replace with your API key
)

# Create FAISS vector store
print("\nCreating FAISS vector store...")
vectorstore = FAISS.from_documents(
    documents=final_chunks,
    embedding=embeddings
)

print("Vector store created successfully!")

# Save vector store for future use (optional)
# vectorstore.save_local("faiss_index")

# ============================================================================
# STEP 4: HYBRID SEARCH - COMBINE VECTOR + KEYWORD SEARCH
# ============================================================================

# Create vector retriever with MMR for diversity
vector_retriever = vectorstore.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance
    search_kwargs={
        "k": 5,  # Retrieve top 5 chunks
        "fetch_k": 20,  # Consider 20 candidates
        "lambda_mult": 0.7  # Balance between relevance and diversity
    }
)

# Create BM25 (keyword) retriever
bm25_retriever = BM25Retriever.from_documents(final_chunks)
bm25_retriever.k = 5

# Combine both retrievers with ensemble
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]  # 70% vector, 30% keyword
)

print("Hybrid retrieval system configured!")

# ============================================================================
# STEP 5: ADD RERANKING FOR BETTER ACCURACY
# ============================================================================

# Initialize LLM for compression/reranking
llm_for_compression = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key="YOUR_API_KEY",
    temperature=0
)

# Create compressor
compressor = LLMChainExtractor.from_llm(llm_for_compression)

# Create compression retriever
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=ensemble_retriever
)

print("Reranking system configured!")

# ============================================================================
# STEP 6: SETUP QA CHAIN WITH CUSTOM PROMPT
# ============================================================================

# Custom prompt for maximum accuracy
prompt_template = """You are an expert assistant for GCM (Global Compliance Management) and GIAM (Global Identity and Access Management) procedures.

Use the following context to answer the question accurately and comprehensively.

Context: {context}

Question: {question}

Instructions:
- Provide accurate information based ONLY on the context provided
- Cite specific section headers when referencing procedures
- Maintain exact timeframes (business days) mentioned in the documentation
- Preserve escalation levels and hierarchies (IC, ESC1-ESC4)
- Include all required participants/stakeholders mentioned
- If the answer involves a procedure, list the steps in order
- If timeframes are mentioned, include them explicitly
- If you're not completely certain, acknowledge the uncertainty
- Do not make up or infer information not in the context

Answer:"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# Initialize Gemini 2.5 Flash with optimal settings
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key="YOUR_API_KEY",
    temperature=0,  # Set to 0 for maximum accuracy and consistency
    max_output_tokens=2048,
    convert_system_message_to_human=True
)

# Create QA chain with compression retriever
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # Stuff all context into prompt
    retriever=compression_retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": PROMPT}
)

print("QA Chain ready!")

# ============================================================================
# STEP 7: TESTING AND EVALUATION
# ============================================================================

def query_with_details(question):
    """Enhanced query function with detailed output"""
    print(f"\n{'='*80}")
    print(f"QUESTION: {question}")
    print(f"{'='*80}\n")
    
    result = qa_chain.invoke({"query": question})
    
    print("ANSWER:")
    print(result['result'])
    
    print(f"\n{'-'*80}")
    print("SOURCE DOCUMENTS:")
    print(f"{'-'*80}\n")
    
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"--- Source {i} ---")
        print(f"Content Preview: {doc.page_content[:300]}...")
        print(f"\nMetadata:")
        for key, value in doc.metadata.items():
            if value:  # Only show non-empty metadata
                print(f"  {key}: {value}")
        print()
    
    return result

# ============================================================================
# SAMPLE TEST QUERIES
# ============================================================================

# Test Query 1: Escalation timeframes
test_query_1 = "What are the escalation timeframes for ESC2 level?"
result1 = query_with_details(test_query_1)

# Test Query 2: Ticket types
test_query_2 = "What ticket types require manager approval?"
result2 = query_with_details(test_query_2)

# Test Query 3: Procedures
test_query_3 = "Explain the Segregation of Duties procedure"
result3 = query_with_details(test_query_3)

# Test Query 4: Participants
test_query_4 = "Who should be CC'd on manual escalations?"
result4 = query_with_details(test_query_4)

# ============================================================================
# OPTIONAL: SAVE AND LOAD FUNCTIONALITY
# ============================================================================

# Save vector store
def save_vectorstore(vectorstore, path="./faiss_gcm_index"):
    """Save FAISS vector store to disk"""
    vectorstore.save_local(path)
    print(f"Vector store saved to {path}")

# Load vector store
def load_vectorstore(path="./faiss_gcm_index"):
    """Load FAISS vector store from disk"""
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key="YOUR_API_KEY"
    )
    vectorstore = FAISS.load_local(path, embeddings)
    print(f"Vector store loaded from {path}")
    return vectorstore

# Example usage:
# save_vectorstore(vectorstore)
# vectorstore = load_vectorstore()

print("\n" + "="*80)
print("SETUP COMPLETE! Ready for queries.")
print("="*80)




