import atexit
import os
import tempfile
import streamlit as st
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from vertex_langchain import VertexClientLLM, VertexClientEmbeddings
import re
from typing import List, Dict, Set
import hashlib
import json
from datetime import datetime
import pandas as pd
from docx import Document as DocxDocument
import PyPDF2

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Load environment and secrets

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

load_dotenv()
from utils import load_secrets, authenticate
load_secrets()

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Constants

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

FAISS_INDEX_DIR = ‚Äúfaiss_index‚Äù
METADATA_CACHE = ‚Äúprocessed_files.json‚Äù  # Changed to JSON for richer metadata
QUERY_HISTORY_FILE = ‚Äúquery_history.json‚Äù
FEEDBACK_FILE = ‚Äúuser_feedback.json‚Äù
USER_CONFIG_FILE = ‚Äúuser_config.json‚Äù

os.makedirs(FAISS_INDEX_DIR, exist_ok=True)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Initialize Vertex clients

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

client = VertexClientLLM()
embeddings = VertexClientEmbeddings(client)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# File Hash & Metadata Management

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

def get_processed_files() -> Dict:
‚Äú‚Äù‚ÄúReads processed file metadata from cache.‚Äù‚Äù‚Äù
if not os.path.exists(METADATA_CACHE):
return {}
with open(METADATA_CACHE, ‚Äúr‚Äù) as f:
return json.load(f)

def save_processed_files(metadata: Dict):
‚Äú‚Äù‚ÄúSaves processed file metadata to cache.‚Äù‚Äù‚Äù
with open(METADATA_CACHE, ‚Äúw‚Äù) as f:
json.dump(metadata, f, indent=2)

def get_file_hash(file_content: bytes) -> str:
‚Äú‚Äù‚ÄúGenerates SHA256 hash for file content.‚Äù‚Äù‚Äù
return hashlib.sha256(file_content).hexdigest()

def add_to_processed_files(file_hash: str, filename: str, doc_count: int):
‚Äú‚Äù‚ÄúAdds new file metadata to cache.‚Äù‚Äù‚Äù
metadata = get_processed_files()
metadata[file_hash] = {
‚Äúfilename‚Äù: filename,
‚Äúprocessed_date‚Äù: datetime.now().isoformat(),
‚Äúchunk_count‚Äù: doc_count
}
save_processed_files(metadata)

def remove_from_processed_files(file_hash: str):
‚Äú‚Äù‚ÄúRemoves file from cache (for deletions).‚Äù‚Äù‚Äù
metadata = get_processed_files()
if file_hash in metadata:
del metadata[file_hash]
save_processed_files(metadata)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Query History & Feedback Management

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

def log_query(query: str, answer: str, sources: List[Dict], user: str = ‚Äúdefault‚Äù):
‚Äú‚Äù‚ÄúLogs user queries for audit and analytics.‚Äù‚Äù‚Äù
history = []
if os.path.exists(QUERY_HISTORY_FILE):
with open(QUERY_HISTORY_FILE, ‚Äúr‚Äù) as f:
history = json.load(f)


history.append({
    "timestamp": datetime.now().isoformat(),
    "user": user,
    "query": query,
    "answer": answer,
    "sources": sources
})

with open(QUERY_HISTORY_FILE, "w") as f:
    json.dump(history, f, indent=2)


def log_feedback(query: str, answer: str, rating: str, comment: str = ‚Äú‚Äù, user: str = ‚Äúdefault‚Äù):
‚Äú‚Äù‚ÄúLogs user feedback for continuous improvement.‚Äù‚Äù‚Äù
feedback = []
if os.path.exists(FEEDBACK_FILE):
with open(FEEDBACK_FILE, ‚Äúr‚Äù) as f:
feedback = json.load(f)


feedback.append({
    "timestamp": datetime.now().isoformat(),
    "user": user,
    "query": query,
    "answer": answer,
    "rating": rating,
    "comment": comment
})

with open(FEEDBACK_FILE, "w") as f:
    json.dump(feedback, f, indent=2)


def get_query_analytics() -> pd.DataFrame:
‚Äú‚Äù‚ÄúReturns query analytics for business analysts.‚Äù‚Äù‚Äù
if not os.path.exists(QUERY_HISTORY_FILE):
return pd.DataFrame()


with open(QUERY_HISTORY_FILE, "r") as f:
    history = json.load(f)

return pd.DataFrame(history)


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# User Access Management

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

def check_user_access(username: str) -> bool:
‚Äú‚Äù‚ÄúSimple user access check (expand with SSO/LDAP in production).‚Äù‚Äù‚Äù
if not os.path.exists(USER_CONFIG_FILE):
# Default: allow all users in development
return True


with open(USER_CONFIG_FILE, "r") as f:
    config = json.load(f)
    return username in config.get("allowed_users", [])


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Multi-Format Document Processing

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

class MultiFormatPCMProcessor:
‚Äú‚Äù‚ÄúEnhanced processor supporting DOCX, PDF, and Markdown‚Äù‚Äù‚Äù


@staticmethod
def extract_text_from_docx(file_path: str) -> str:
    """Extract text from DOCX files."""
    doc = DocxDocument(file_path)
    text = []
    
    for para in doc.paragraphs:
        text.append(para.text)
    
    # Extract tables
    for table in doc.tables:
        text.append("\n[TABLE START]")
        for row in table.rows:
            row_data = [cell.text for cell in row.cells]
            text.append(" | ".join(row_data))
        text.append("[TABLE END]\n")
    
    return "\n".join(text)

@staticmethod
def extract_text_from_pdf(file_path: str) -> str:
    """Extract text from PDF files."""
    text = []
    
    with open(file_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text.append(page.extract_text())
    
    return "\n".join(text)

@staticmethod
def extract_text_from_markdown(file_path: str) -> str:
    """Extract text from Markdown files."""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

@staticmethod
def extract_document_structure(text: str) -> Dict:
    """Extract hierarchical structure from document."""
    lines = text.split('\n')
    structure = {
        'title': '',
        'sections': [],
        'current_section': None
    }
    
    for line in lines:
        if line.startswith('# ') and not structure['title']:
            structure['title'] = line[2:].strip()
        elif line.startswith('## '):
            structure['current_section'] = line[3:].strip()
            structure['sections'].append(structure['current_section'])
    
    return structure

@staticmethod
def convert_tables_to_structured_text(text: str) -> str:
    """Convert markdown tables to structured text."""
    table_pattern = r'(\|.+?\|(?:\n\|[-:\s]+\|)+\n(?:\|.*\|)+)'
    tables = re.findall(table_pattern, text, flags=re.DOTALL)
    
    for table in tables:
        rows = [r.strip() for r in table.strip().split('\n') if r.strip()]
        headers = [h.strip() for h in rows[0].split('|') if h.strip()]
        data_rows = rows[2:]
        
        table_text = "\n[TABLE START]\n"
        table_text += f"Columns: {', '.join(headers)}\n"
        
        for idx, row in enumerate(data_rows, 1):
            values = [v.strip() for v in row.split('|') if v.strip()]
            if len(values) == len(headers):
                row_dict = dict(zip(headers, values))
                table_text += f"Row {idx}: " + " | ".join(
                    f"{k}={v}" for k, v in row_dict.items()
                ) + "\n"
        
        table_text += "[TABLE END]\n"
        text = text.replace(table, table_text)
    
    return text

@staticmethod
def add_contextual_metadata(chunks: List[str], structure: Dict, 
                             doc_name: str, file_hash: str) -> List[Document]:
    """Add rich metadata to chunks."""
    documents = []
    current_section = structure.get('title', 'Unknown')
    
    for i, chunk in enumerate(chunks):
        section_match = re.search(r'##\s+(.+?)(?:\n|$)', chunk)
        if section_match:
            current_section = section_match.group(1).strip()
        
        metadata = {
            'source': doc_name,
            'file_hash': file_hash,
            'chunk_id': i,
            'section': current_section,
            'doc_title': structure.get('title', doc_name),
            'char_count': len(chunk),
            'processed_date': datetime.now().isoformat()
        }
        
        documents.append(Document(page_content=chunk, metadata=metadata))
    
    return documents


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Streamlit Page

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

st.set_page_config(page_title=‚ÄúGCM PCM Assistant‚Äù, layout=‚Äúwide‚Äù)

# Simple authentication (expand with SSO in production)

if ‚Äòuser‚Äô not in st.session_state:
st.session_state.user = ‚Äúanalyst_user‚Äù

if not check_user_access(st.session_state.user):
st.error(‚Äúüö´ Access Denied. Contact administrator.‚Äù)
st.stop()

st.title(‚Äúüîç GCM Process Control Manual Assistant‚Äù)
st.markdown(f‚Äù*Welcome, {st.session_state.user}* | Real-time Intelligence Search for PCMs & Policy Documents‚Äù)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Sidebar Configuration

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

with st.sidebar:
st.header(‚Äú‚öôÔ∏è Configuration‚Äù)


tab1, tab2, tab3 = st.tabs(["Settings", "Analytics", "Admin"])

with tab1:
    chunk_size = st.slider("Chunk Size", 400, 1000, 700)
    chunk_overlap = st.slider("Chunk Overlap", 50, 200, 150)
    retrieval_k = st.slider("Retrieved Chunks", 3, 10, 5)
    
    st.markdown("---")
    st.markdown("### üìä Index Status")
    if os.path.exists(FAISS_INDEX_DIR):
        st.success("‚úÖ Index Active")
        metadata = get_processed_files()
        st.info(f"üìÑ {len(metadata)} document(s)")
        total_chunks = sum(m.get('chunk_count', 0) for m in metadata.values())
        st.info(f"üì¶ {total_chunks} total chunks")
    else:
        st.warning("‚ö†Ô∏è No index found")

with tab2:
    st.markdown("### üìà Usage Analytics")
    if os.path.exists(QUERY_HISTORY_FILE):
        df = get_query_analytics()
        st.metric("Total Queries", len(df))
        
        if len(df) > 0:
            st.markdown("**Recent Activity**")
            recent = df.tail(5)[['timestamp', 'query']]
            st.dataframe(recent, use_container_width=True)
    else:
        st.info("No query history yet")

with tab3:
    st.markdown("### üîß Admin Tools")
    if st.button("üóëÔ∏è Clear All Data"):
        import shutil
        if os.path.exists(FAISS_INDEX_DIR):
            shutil.rmtree(FAISS_INDEX_DIR)
        for file in [METADATA_CACHE, QUERY_HISTORY_FILE, FEEDBACK_FILE]:
            if os.path.exists(file):
                os.remove(file)
        st.success("‚úÖ All data cleared!")
        st.rerun()
    
    if st.button("üìä Export Analytics"):
        if os.path.exists(QUERY_HISTORY_FILE):
            df = get_query_analytics()
            csv = df.to_csv(index=False)
            st.download_button("Download CSV", csv, "query_analytics.csv")


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Main Tabs

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

main_tab1, main_tab2, main_tab3, main_tab4 = st.tabs([
‚Äúüì§ Upload Documents‚Äù,
‚Äúüí¨ Ask Questions‚Äù,
‚Äúüìã Document Summaries‚Äù,
‚Äúüìä Query History‚Äù
])

processor = MultiFormatPCMProcessor()

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Tab 1: Upload Documents

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

with main_tab1:
st.header(‚ÄúüìÑ Upload PCM Documents‚Äù)
st.markdown(‚ÄúSupports: *DOCX, PDF, Markdown* | Max ~350 documents | 5-60 pages each‚Äù)


uploaded_files = st.file_uploader(
    "Choose document(s)", 
    type=["md", "docx", "pdf"], 
    accept_multiple_files=True
)

if uploaded_files:
    st.success(f"‚úÖ {len(uploaded_files)} file(s) uploaded")
    
    with st.expander("üìã File List"):
        for file in uploaded_files:
            file_type = file.name.split('.')[-1].upper()
            st.write(f"- {file.name} ({file_type})")
    
    if st.button("üöÄ Process & Index Documents", type="primary"):
        all_new_documents = []
        processed_files_cache = get_processed_files()
        new_files_processed = 0
        skipped_files = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, uploaded_file in enumerate(uploaded_files):
            file_content = uploaded_file.getvalue()
            file_hash = get_file_hash(file_content)
            
            if file_hash in processed_files_cache:
                status_text.text(f"‚è≠Ô∏è Skipping {uploaded_file.name} (already processed)")
                skipped_files.append(uploaded_file.name)
                progress_bar.progress((idx + 1) / len(uploaded_files))
                continue
            
            status_text.text(f"üîÑ Processing {uploaded_file.name}...")
            new_files_processed += 1
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{uploaded_file.name}") as temp_file:
                temp_file.write(file_content)
                file_path = temp_file.name
            
            atexit.register(lambda p=file_path: os.remove(p) if os.path.exists(p) else None)
            
            # Extract text based on file type
            file_ext = uploaded_file.name.split('.')[-1].lower()
            
            if file_ext == 'docx':
                text = processor.extract_text_from_docx(file_path)
            elif file_ext == 'pdf':
                text = processor.extract_text_from_pdf(file_path)
            else:  # markdown
                text = processor.extract_text_from_markdown(file_path)
            
            structure = processor.extract_document_structure(text)
            text = processor.convert_tables_to_structured_text(text)
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n## ", "\n### ", "\n\n", "\n", ". ", " ", ""],
                length_function=len,
            )
            
            chunks = text_splitter.split_text(text)
            docs = processor.add_contextual_metadata(chunks, structure, uploaded_file.name, file_hash)
            
            all_new_documents.extend(docs)
            add_to_processed_files(file_hash, uploaded_file.name, len(docs))
            
            progress_bar.progress((idx + 1) / len(uploaded_files))
        
        if all_new_documents:
            status_text.text(f"üíæ Indexing {len(all_new_documents)} chunks...")
            
            if os.path.exists(FAISS_INDEX_DIR):
                faiss_store = FAISS.load_local(FAISS_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
                faiss_store.add_documents(all_new_documents)
            else:
                faiss_store = FAISS.from_documents(all_new_documents, embeddings)
            
            faiss_store.save_local(FAISS_INDEX_DIR)
            
            msg = f"‚úÖ Processed {new_files_processed} new document(s) | {len(all_new_documents)} chunks indexed"
            if skipped_files:
                msg += f"\n\n‚è≠Ô∏è Skipped {len(skipped_files)} duplicate(s)"
            st.success(msg)
        else:
            st.info("‚ÑπÔ∏è All documents already processed")
        
        status_text.empty()
        progress_bar.empty()


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Tab 2: Ask Questions

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

with main_tab2:
st.header(‚Äú‚ùì Ask Questions About PCMs‚Äù)


col1, col2 = st.columns([3, 1])

with col1:
    user_query = st.text_area(
        "Your Question:",
        placeholder="e.g., What are the validation procedures for application XYZ?\nWho should I contact for compliance issues?",
        height=100
    )

with col2:
    st.markdown("**üí° Examples**")
    st.markdown("""
    - Summarize PCM for app X
    - What are quality controls?
    - Who handles escalations?
    - List all monitoring steps
    """)

if user_query and st.button("üîç Get Answer", type="primary"):
    if not os.path.exists(FAISS_INDEX_DIR):
        st.error("‚ö†Ô∏è No index found. Upload documents first.")
    else:
        with st.spinner("üîÑ Searching..."):
            faiss_store = FAISS.load_local(FAISS_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
            results = faiss_store.similarity_search_with_score(user_query, k=retrieval_k)
        
        if results:
            context_parts = []
            sources = []
            
            for doc, score in results:
                source_info = f"[{doc.metadata.get('source', 'Unknown')} - {doc.metadata.get('section', 'N/A')}]"
                context_parts.append(f"{source_info}\n{doc.page_content}")
                sources.append({
                    'source': doc.metadata.get('source', 'Unknown'),
                    'section': doc.metadata.get('section', 'N/A'),
                    'similarity': f"{(1-score)*100:.1f}%"
                })
            
            context = "\n\n---\n\n".join(context_parts)
            
            prompt = f"""You are a GCM Process Control Manual expert analyst. Answer questions with 90%+ accuracy.


Question: {user_query}

Context from PCM documents:
{context}

Instructions:

1. Provide comprehensive answers synthesizing all relevant sections
1. Include step-by-step procedures when applicable
1. Extract and present table data clearly
1. Identify contact persons or escalation paths if mentioned
1. Cite sources as [Document - Section]
1. State explicitly if information is incomplete
1. Maintain strict accuracy - never infer beyond the context

Answer:‚Äù‚Äù‚Äù


            with st.spinner("ü§î Generating answer..."):
                answer = client(prompt)
            
            st.subheader("üí° Answer:")
            st.markdown(answer)
            
            # Log query
            log_query(user_query, answer, sources, st.session_state.user)
            
            st.subheader("üìö Sources:")
            for i, source in enumerate(sources, 1):
                st.markdown(f"**{i}.** {source['source']} - *{source['section']}* ({source['similarity']})")
            
            # Feedback mechanism
            st.markdown("---")
            st.subheader("üìù Feedback")
            col_fb1, col_fb2, col_fb3 = st.columns([1, 1, 3])
            
            with col_fb1:
                if st.button("üëç Helpful"):
                    log_feedback(user_query, answer, "positive", "", st.session_state.user)
                    st.success("Thank you!")
            
            with col_fb2:
                if st.button("üëé Not Helpful"):
                    log_feedback(user_query, answer, "negative", "", st.session_state.user)
                    st.warning("Feedback logged")
            
            with col_fb3:
                correction = st.text_input("üí¨ Correction/Comment (optional):")
                if correction and st.button("Submit Comment"):
                    log_feedback(user_query, answer, "comment", correction, st.session_state.user)
                    st.success("Comment logged!")
            
            with st.expander("üîç View Retrieved Chunks"):
                for i, (doc, score) in enumerate(results, 1):
                    st.markdown(f"### Chunk {i}")
                    st.markdown(f"**Source:** {doc.metadata.get('source')}")
                    st.markdown(f"**Section:** {doc.metadata.get('section')}")
                    st.markdown(f"**Relevance:** {(1-score)*100:.1f}%")
                    st.text_area(f"Content", doc.page_content, height=150, key=f"chunk_{i}")
                    st.markdown("---")
        else:
            st.warning("‚ö†Ô∏è No relevant information found")


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Tab 3: Document Summaries

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

with main_tab3:
st.header(‚Äúüìã Generate Document Summaries‚Äù)
st.markdown(‚ÄúGet high-level overviews of specific PCMs‚Äù)


metadata = get_processed_files()
if metadata:
    doc_names = list(set(m['filename'] for m in metadata.values()))
    selected_doc = st.selectbox("Select a document to summarize:", doc_names)
    
    if st.button("üìÑ Generate Summary"):
        with st.spinner("Generating summary..."):
            faiss_store = FAISS.load_local(FAISS_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
            
            # Get all chunks from this document
            all_docs = faiss_store.similarity_search(selected_doc, k=20)
            doc_chunks = [d for d in all_docs if d.metadata.get('source') == selected_doc]
            
            if doc_chunks:
                combined_content = "\n\n".join([d.page_content for d in doc_chunks[:10]])
                
                summary_prompt = f"""Provide a comprehensive summary of this PCM document:


Document: {selected_doc}

Content:
{combined_content}

Create a structured summary including:

1. Document Purpose
1. Key Processes/Procedures
1. Important Contacts/Escalation Paths
1. Compliance Requirements
1. Critical Tables/Data

Summary:‚Äù‚Äù‚Äù


                summary = client(summary_prompt)
                st.markdown(summary)
            else:
                st.warning("No content found for this document")
else:
    st.info("No documents indexed yet")


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

# Tab 4: Query History

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

with main_tab4:
st.header(‚Äúüìä Query History & Analytics‚Äù)
st.markdown(‚ÄùFor Business Analysts: Identify information gaps and documentation needs‚Äù)


if os.path.exists(QUERY_HISTORY_FILE):
    df = get_query_analytics()
    
    st.subheader(f"üìà Total Queries: {len(df)}")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Recent Queries**")
        recent_df = df[['timestamp', 'user', 'query']].tail(10)
        st.dataframe(recent_df, use_container_width=True)
    
    with col2:
        st.markdown("**Most Queried Topics**")
        if len(df) > 0:
            # Simple word frequency analysis
            all_queries = " ".join(df['query'].tolist()).lower()
            words = re.findall(r'\b\w{4,}\b', all_queries)
            from collections import Counter
            top_words = Counter(words).most_common(10)
            st.write({word: count for word, count in top_words})
    
    if st.button("üì• Download Full History"):
        csv = df.to_csv(index=False)
        st.download_button("Download CSV", csv, "full_query_history.csv")
else:
    st.info("No query history available yet")


st.markdown(‚Äù‚Äî‚Äù)
st.markdown(‚Äù<div style='text-align: center; color: gray;'>GCM PCM Assistant v3.0 - Enterprise RAG Solution</div>‚Äù, unsafe_allow_html=True)
