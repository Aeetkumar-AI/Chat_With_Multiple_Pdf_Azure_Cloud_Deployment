import os, glob, json
from typing import List, Dict
from langchain.docstore.document import Document
from langchain.chat_models import ChatVertexAI
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain.embeddings import VertexAIEmbeddings

##############################################################
# CONFIG
##############################################################
DOCUMENTS_DIR = "Doc"
VECTOR_DIR = "vectorstore"
CHUNK_SIZE = 600
OVERLAP = 120
TOP_K = 30

##############################################################
# MODELS
##############################################################
llm = ChatVertexAI(
    model="gemini-2.0-pro",
    temperature=0,
    max_output_tokens=1200
)

embeddings = VertexAIEmbeddings()

##############################################################
# 1ï¸âƒ£ LOAD RAW DOCS
##############################################################
def load_docs(path: str) -> List[Document]:
    docs = []
    for file in glob.glob(os.path.join(path, "*.md")):
        loader = UnstructuredMarkdownLoader(file)
        for d in loader.load():
            d.metadata["source_file"] = os.path.basename(file)
            docs.append(d)
    print(f"[+] Loaded {len(docs)} md docs")
    return docs


##############################################################
# 2ï¸âƒ£ SEMANTIC METADATA PROMPT
##############################################################
META_PROMPT = """
Your task is to extract structured metadata for compliance/security/process manuals.

Return STRICT JSON:
{
 "section_title": string,
 "semantic_topic": string,
 "business_domain": string,
 "document_version": string,
 "effective_date": string,
 "risk_level": "low|medium|high",
 "keywords": [string,string,string],
 "summary": string
}

RULES:
- Extract only FACTUAL data from text.
- Never hallucinate.
- If unknown â†’ "" or [].
- Keywords must be short domain terms (max 2 words).
- Prefer operational/technical terms.
- Risk HIGH if production access, privileges, escalation.
"""

def llm_metadata(doc: Document):
    content = doc.page_content[:1800]
    resp = llm.invoke([
        {"role":"system","content":"You output only JSON."},
        {"role":"user","content": META_PROMPT + "\n\n" + content}
    ])
    try:
        meta = json.loads(resp.content)
        doc.metadata.update(meta)
    except:
        doc.metadata.update({
            "section_title":"",
            "semantic_topic":"uncategorized",
            "business_domain":"",
            "document_version":"",
            "effective_date":"",
            "risk_level":"low",
            "keywords":[],
            "summary":""
        })


##############################################################
# 3ï¸âƒ£ CHUNKING A â€” STRUCTURAL / RECURSIVE
##############################################################
def structural_chunks(docs: List[Document]) -> List[Document]:
    header_splitter = MarkdownHeaderTextSplitter(
        headers=[("#","H1"),("##","H2"),("###","H3"),("####","H4")],
        strip_headers=False,
    )
    rec_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=OVERLAP,
        separators=["\n\n","\n"," "]
    )
    out=[]
    for d in docs:
        try:
            parts = header_splitter.split_text(d.page_content)
        except:
            parts = [d]
        for p in rec_splitter.split_documents(parts):
            p.metadata["source_file"]=d.metadata["source_file"]
            p.metadata["chunk_type"]="recursive"
            out.append(p)
    print(f"[A] Structural chunks = {len(out)}")
    return out


##############################################################
# 4ï¸âƒ£ CHUNKING B â€” AGENTIC (LLM)
##############################################################
AGENTIC_PROMPT = """
Split the document into semantically complete chunks.
Follow these HARD RULES:
- Do NOT split steps, tables, workflows
- Do NOT rewrite text
- Every chunk is one complete idea
- Keep formatting
Return chunks separated by:
===CHUNK===
"""

def agentic_chunk_single(text:str)->List[str]:
    resp = llm.invoke([
        {"role":"system","content":AGENTIC_PROMPT},
        {"role":"user","content":text}
    ])
    raw = resp.content
    return [c.strip() for c in raw.split("===CHUNK===") if c.strip()]

def agentic_chunks(docs:List[Document])->List[Document]:
    out=[]
    for d in docs:
        parts=agentic_chunk_single(d.page_content)
        for i,ch in enumerate(parts,1):
            nd=Document(
                page_content=ch,
                metadata={
                    "source_file":d.metadata["source_file"],
                    "chunk_index":i,
                    "chunk_type":"agentic"
                }
            )
            out.append(nd)
    print(f"[B] Agentic chunks = {len(out)}")
    return out


##############################################################
# 5ï¸âƒ£ ENRICH METADATA FOR ALL CHUNKS
##############################################################
def enrich(chunks:List[Document]):
    for d in chunks:
        llm_metadata(d)
    print("[âœ“] Metadata enrichment complete")


##############################################################
# 6ï¸âƒ£ BUILD VECTORSTORES (A/B)
##############################################################
def build_vstores(chunksA, chunksB):
    VSA = FAISS.from_documents(chunksA, embeddings)
    VSB = FAISS.from_documents(chunksB, embeddings)

    os.makedirs(VECTOR_DIR,exist_ok=True)
    VSA.save_local(f"{VECTOR_DIR}/A")
    VSB.save_local(f"{VECTOR_DIR}/B")
    print("[âœ“] Vectorstores saved")

    return VSA, VSB


##############################################################
# 7ï¸âƒ£ LOAD VECTORSTORES
##############################################################
def load_vstores():
    VSA = FAISS.load_local(f"{VECTOR_DIR}/A", embeddings, allow_dangerous_deserialization=True)
    VSB = FAISS.load_local(f"{VECTOR_DIR}/B", embeddings, allow_dangerous_deserialization=True)
    print("[âœ“] Vectorstores loaded")
    return VSA, VSB


##############################################################
# 8ï¸âƒ£ QUERY REWRITING
##############################################################
REWRITE_PROMPT = """
Rewrite the user query into a precise compliance/security search query.
Do NOT add assumptions.
Do NOT invent terms.
Return 1 rewritten query string.
"""

def rewrite_query(q:str)->str:
    resp = llm.invoke([
        {"role":"system","content":REWRITE_PROMPT},
        {"role":"user","content":q}
    ])
    return resp.content.strip()


##############################################################
# 9ï¸âƒ£ HIERARCHICAL HYBRID RETRIEVAL
##############################################################
def retrieve(query:str, VSA, VSB, k=TOP_K):
    q2 = rewrite_query(query)
    print(f"[Q] rewritten = {q2}")

    rA1 = VSA.similarity_search(q2,k)
    rA2 = VSA.max_marginal_relevance_search(q2,k,fetch_k=2*k)
    rB1 = VSB.similarity_search(q2,k)
    rB2 = VSB.max_marginal_relevance_search(q2,k,fetch_k=2*k)

    combined = rA1+rA2+rB1+rB2

    uniq=[]
    seen=set()
    for d in combined:
        if d.page_content not in seen:
            uniq.append(d)
            seen.add(d.page_content)

    print(f"[+] Retrieved unique chunks: {len(uniq)}")
    return uniq[:2*k]


##############################################################
# ðŸ”Ÿ BUILD ANSWER
##############################################################
ANSWER_PROMPT = """
You are an expert PCM/GCM/IAM analyst.
Use ONLY the provided context.
Follow strict compliance formatting.

Rules:
- Do not hallucinate
- Do not invent missing policy lines
- Extract tables/steps clearly
- Provide escalation paths if present
- Reference sections if provided
- No HTML, no markdown tables

Return a structured actionable answer.
"""

def build_answer(context_docs:List[Document], query:str)->str:
    ctx="\n\n---\n\n".join(
        f"[{i+1}] {d.metadata.get('source_file','')} | {d.metadata.get('section_title','')}\n{d.page_content}"
        for i,d in enumerate(context_docs)
    )
    resp = llm.invoke([
        {"role":"system","content":ANSWER_PROMPT},
        {"role":"user","content":f"Context:\n{ctx}"},
        {"role":"user","content":f"Question:\n{query}"}
    ])
    return resp.content


##############################################################
# 11ï¸âƒ£ ASK
##############################################################
def ask(q:str)->str:
    VSA,VSB = load_vstores()
    docs = retrieve(q,VSA,VSB)
    return build_answer(docs,q)


##############################################################
# PIPELINE RUN
##############################################################
if __name__=="__main__":
    raw = load_docs(DOCUMENTS_DIR)
    A = structural_chunks(raw)
    B = agentic_chunks(raw)
    enrich(A)
    enrich(B)
    build_vstores(A,B)

    # test
    print(ask("What is production access policy?"))
