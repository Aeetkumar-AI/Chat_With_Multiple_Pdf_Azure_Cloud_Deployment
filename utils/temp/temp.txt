from langchain.schema import Document, SystemMessage, HumanMessage
import json

def agentic_chunking_with_metadata(text, llm, source_file):
    """
    Dynamically splits text into meaningful chunks using LLM.
    
    Args:
        text: Document text to chunk
        llm: LangChain LLM instance
        source_file: Path to source file
        
    Returns:
        List of Document objects with content and basic metadata
    """
    
    system_message = SystemMessage(content=CHUNKING_SYSTEM_PROMPT)
    
    human_message = HumanMessage(content=f"""
Document to chunk:

{text}
""")
    
    try:
        # Get LLM response
        response = llm.invoke([system_message, human_message])
        
        # Extract content from response
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        # Clean response (remove markdown code blocks if present)
        response_text = response_text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        elif response_text.startswith('```'):
            response_text = response_text[3:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        # Parse JSON response
        parsed_response = json.loads(response_text)
        
        # Handle both dict and list responses
        if isinstance(parsed_response, dict):
            # If response has document_metadata and chunks structure
            chunks_data = parsed_response.get("chunks", [])
            doc_metadata = parsed_response.get("document_metadata", {})
        elif isinstance(parsed_response, list):
            # If response is directly a list of chunks
            chunks_data = parsed_response
            doc_metadata = {}
        else:
            raise ValueError("Unexpected response format")
        
        # Create Document objects
        documents = []
        for chunk_data in chunks_data:
            # Extract chunk content and metadata
            if isinstance(chunk_data, dict):
                content = chunk_data.get("content", "")
                chunk_id = chunk_data.get("chunk_id", len(documents) + 1)
                chunk_metadata = chunk_data.get("metadata", {})
            else:
                # Fallback if chunk_data is just a string
                content = str(chunk_data)
                chunk_id = len(documents) + 1
                chunk_metadata = {}
            
            # Create simplified metadata
            metadata = {
                "source": source_file,
                "chunk_id": chunk_id,
                **chunk_metadata  # Add any chunk-level metadata if present
            }
            
            # Create Document object
            doc = Document(
                page_content=content,
                metadata=metadata
            )
            documents.append(doc)
        
        print(f"‚úÖ Successfully chunked into {len(documents)} chunks from {source_file}")
        return documents
        
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON Error parsing LLM response: {e}")
        print(f"Response preview: {response_text[:500]}")
        
        # Fallback: split by double newlines
        chunks = [chunk.strip() for chunk in text.split("\n\n") if chunk.strip()]
        documents = []
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk,
                metadata={
                    "source": source_file,
                    "chunk_id": i + 1,
                    "chunk_type": "fallback"
                }
            )
            documents.append(doc)
        
        print(f"‚ö†Ô∏è Fallback chunking: Created {len(documents)} chunks")
        return documents
        
    except Exception as e:
        print(f"‚ùå Error during chunking: {e}")
        # Return original document as single chunk
        return [Document(
            page_content=text,
            metadata={
                "source": source_file,
                "chunk_id": 1,
                "chunk_type": "error_fallback"
            }
        )]


# =====================================================
# PROCESS ALL DOCUMENTS
# =====================================================

# Initialize LLM
llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=0)

# Process all loaded documents
chunked_docs = []
for i, d in enumerate(docs, 1):
    print(f"\nüìÑ Processing document {i}/{len(docs)}")
    
    file_content = d.page_content
    source_file = d.metadata.get("source", f"unknown_source_{i}")
    
    chunks = agentic_chunking_with_metadata(
        text=file_content,
        llm=llm,
        source_file=source_file
    )
    chunked_docs.extend(chunks)

print(f"\nüéâ Total chunks created: {len(chunked_docs)}")

# =====================================================
# CREATE VECTOR STORE
# =====================================================

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunked_docs, embeddings)
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 7})

print(f"‚úÖ Vector store created with {len(chunked_docs)} chunks")

# =====================================================
# SETUP RAG CHAIN
# =====================================================

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an expert GCM (Global Compliance Monitoring) and PCM (Process Controls Manual) analyst with comprehensive knowledge of GIAM Operations, security report reconciliation, escalation procedures, and compliance monitoring processes.

Instructions for Expert Analysis:
1. Provide comprehensive answers synthesizing all relevant information
2. Include specific step-by-step procedures when applicable
3. Extract and present table data with clear formatting
4. Reference specific sections and document sources when available
5. Use clear, concise language with no unnecessary jargon
6. Ensure all answers are actionable and directly address the question asked

Answer in a clear, actionable format that directly addresses the user's question:"""),
    ("human", "Context:\n{context}\n\nQuestion: {question}")
])

def run(query):
    """Run query against vector store"""
    docs = retriever.invoke(query)
    context = "\n\n".join([doc.page_content for doc in docs])
    
    chain = prompt | llm
    inputs = {
        "context": context,
        "question": query
    }
    
    response = chain.invoke(inputs)
    
    # Print sources
    print("\nüìö Sources:")
    for i, doc in enumerate(docs[:3], 1):
        source = doc.metadata.get("source", "Unknown")
        chunk_id = doc.metadata.get("chunk_id", "?")
        print(f"{i}. {source} (Chunk {chunk_id})")
    
    return response.content if hasattr(response, 'content') else response

# =====================================================
# USAGE
# =====================================================

# Example query
result = run("What is the access request process?")
print(f"\nüí¨ Answer:\n{result}")
