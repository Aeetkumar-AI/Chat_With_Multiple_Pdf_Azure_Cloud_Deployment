"""
Single-file pipeline: structural + agentic chunking, LLM metadata enrichment with robust parsing + safe fallback heuristics,
build two FAISS vectorstores and an ask(query) function.

Requirements:
 - langchain (chat models, docstore)
 - langchain_community (document_loaders, vectorstores)
 - langchain_text_splitters
 - Vertex embeddings/chat (or replace with your LLM/embeddings)
"""

import os
import glob
import json
import re
import time
from typing import List
from langchain.docstore.document import Document
from langchain.chat_models import ChatVertexAI
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import VertexAIEmbeddings

# -------------------------
# CONFIG
# -------------------------
DOCUMENTS_DIR = "Doc"
VECTOR_DIR = "vectorstore"
CHUNK_SIZE = 600
CHUNK_OVERLAP = 120
TOP_K = 30
LLM_RETRIES = 2            # number of attempts for LLM metadata parsing
JSON_PARSE_TIMEOUT = 0.25  # pause between parse retries

# -------------------------
# MODELS
# -------------------------
# Replace model name if needed
llm = ChatVertexAI(model="gemini-2.0-pro", temperature=0, max_output_tokens=1200)
embeddings = VertexAIEmbeddings()

# -------------------------
# PROMPTS
# -------------------------
META_PROMPT = """
You are a metadata extraction engine for compliance / PCM / SOP / IAM documents.
Output STRICT JSON only (no prose, no comments, no trailing text).

Return exactly a JSON object with keys:
{
 "section_title": string,
 "semantic_topic": string,
 "business_domain": string,
 "document_version": string,
 "effective_date": string,     // format YYYY-MM-DD or empty
 "risk_level": "low|medium|high",
 "keywords": [string,string,string], // up to 3 short keywords present in text
 "summary": string              // <= 25 words
}

Rules:
- Use only facts present in the provided text chunk.
- If any field can't be determined, use "" or [].
- Keywords must be short terms appearing in the text (1-2 words).
- Risk HIGH means direct production access/privilege/escalation implications.
- Do NOT invent contact names or policies not present.
- Output JSON only. If you must, wrap JSON in text but ensure you include a valid JSON object.

Now process the following text chunk (do not include extra commentary):
"""

AGENTIC_PROMPT = """
Split the document into semantically complete chunks suitable for RAG ingestion.
- Keep headings with their content.
- Do not split tables, code blocks, step lists.
- Preserve formatting.
Return chunks separated by the delimiter exactly: ===CHUNK===
Do NOT output anything else.
"""

REWRITE_PROMPT = """
Rewrite the user's query into a concise search query optimized for document retrieval. 
Do not add assumptions. Return only the rewritten query string.
"""

ANSWER_PROMPT = """
You are an expert PCM/GCM analyst. Use ONLY the provided context chunks to answer.
Be precise, do not hallucinate, and include section references if present.
Return an actionable, structured answer (no HTML).
"""

# -------------------------
# UTIL: robust JSON parse for LLM outputs
# -------------------------
def extract_first_json(text: str) -> str:
    """
    Find the first {...} JSON substring in text.
    Returns the substring or raises ValueError if not found.
    """
    # find the first '{' and matching '}' using stack
    start = text.find("{")
    if start == -1:
        raise ValueError("No JSON object start found")
    stack = []
    for i in range(start, len(text)):
        ch = text[i]
        if ch == "{":
            stack.append(i)
        elif ch == "}":
            if not stack:
                continue
            stack.pop()
            if not stack:
                return text[start:i+1]
    raise ValueError("No complete JSON object found")

def safe_parse_json_from_llm(llm_text: str):
    # Try to extract JSON block then parse.
    try:
        json_text = extract_first_json(llm_text)
        return json.loads(json_text)
    except Exception:
        # Last-resort: try to load whole text (if it's clean JSON)
        try:
            return json.loads(llm_text)
        except Exception:
            raise

# -------------------------
# LOAD RAW DOCUMENTS
# -------------------------
def load_raw_docs(path: str) -> List[Document]:
    docs = []
    for file in glob.glob(os.path.join(path, "*.md")):
        loader = UnstructuredMarkdownLoader(file)
        for d in loader.load():
            d.metadata["source_file"] = os.path.basename(file)
            docs.append(d)
    print(f"[+] Loaded {len(docs)} markdown files from {path}")
    return docs

# -------------------------
# STRUCTURAL CHUNKING (A)
# -------------------------
def structural_chunks(docs: List[Document]) -> List[Document]:
    header_splitter = MarkdownHeaderTextSplitter(
        headers=[("#","H1"),("##","H2"),("###","H3"),("####","H4")],
        strip_headers=False
    )
    rec_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n","\n"," "]
    )
    out=[]
    for d in docs:
        try:
            parts = header_splitter.split_text(d.page_content)
        except Exception:
            parts = [d]
        # parts may be Document objects or plain strings depending on splitter implementation
        # ensure Document objects
        norm_parts=[]
        for p in parts:
            if isinstance(p, Document):
                norm_parts.append(p)
            else:
                norm_parts.append(Document(page_content=p, metadata={"source_file": d.metadata.get("source_file","")}))
        sub = rec_splitter.split_documents(norm_parts)
        for s in sub:
            s.metadata.setdefault("source_file", d.metadata.get("source_file",""))
            s.metadata["chunk_type"]="structural"
            out.append(s)
    print(f"[A] Structural chunks: {len(out)}")
    return out

# -------------------------
# AGENTIC CHUNKING (B)
# -------------------------
def agentic_chunk_text(text: str) -> List[str]:
    # call LLM and split on delimiter
    resp = llm.invoke([
        {"role":"system","content":"You are a chunking assistant."},
        {"role":"user","content": AGENTIC_PROMPT + "\n\n" + text}
    ])
    raw = resp.content if hasattr(resp, "content") else str(resp)
    chunks = [c.strip() for c in raw.split("===CHUNK===") if c.strip()]
    return chunks

def agentic_chunks(docs: List[Document]) -> List[Document]:
    out=[]
    for d in docs:
        parts = agentic_chunk_text(d.page_content)
        for i,p in enumerate(parts,1):
            nd = Document(page_content=p, metadata={
                "source_file": d.metadata.get("source_file",""),
                "chunk_index": i,
                "chunk_type":"agentic"
            })
            out.append(nd)
    print(f"[B] Agentic chunks: {len(out)}")
    return out

# -------------------------
# LLM METADATA ENRICHMENT (with retries + fallback)
# -------------------------
def llm_metadata_enrich(doc: Document):
    # Prepare
    text = doc.page_content
    prompt = META_PROMPT + "\n\n" + text[:4000]  # send up to 4k chars
    last_err = None
    for attempt in range(LLM_RETRIES + 1):
        try:
            resp = llm.invoke([
                {"role":"system","content":"Output only JSON object containing metadata."},
                {"role":"user","content": prompt}
            ])
            llm_text = resp.content if hasattr(resp, "content") else str(resp)
            # parse robustly
            meta = safe_parse_json_from_llm(llm_text)
            # Normalize fields: ensure keys exist
            expected = {
                "section_title":"", "semantic_topic":"", "business_domain":"", "document_version":"",
                "effective_date":"", "risk_level":"low", "keywords":[], "summary":""
            }
            # fill with parsed values where present
            for k,v in expected.items():
                doc.metadata[k] = meta.get(k, v) if isinstance(meta, dict) else v
            # ensure keywords is a list of strings up to 3
            kws = doc.metadata.get("keywords",[])
            if not isinstance(kws, list):
                kws = []
            # trim/clean keywords
            clean_kws = []
            for kk in kws:
                if isinstance(kk, str):
                    s = kk.strip()
                    if s and s.lower() not in [x.lower() for x in clean_kws]:
                        clean_kws.append(s)
                if len(clean_kws) >= 3:
                    break
            doc.metadata["keywords"] = clean_kws
            return
        except Exception as e:
            last_err = e
            time.sleep(JSON_PARSE_TIMEOUT)
            continue

    # If here: LLM attempts failed, apply safe heuristic fallback (regex-based)
    apply_fallback_metadata(doc)
    print(f"[!] Metadata LLM parse failed for chunk (source={doc.metadata.get('source_file')}). Used fallback. Error: {last_err}")

# -------------------------
# Safe heuristic fallback (only for minimal fields)
# -------------------------
def apply_fallback_metadata(doc: Document):
    text = doc.page_content

    # Section title: first H1 or first header line
    title = ""
    m = re.search(r"^#\s+(.+)$", text, flags=re.MULTILINE)
    if m:
        title = m.group(1).strip()
    else:
        # first non-empty line up to 80 chars
        for line in text.splitlines():
            s=line.strip()
            if s:
                title = s[:80]
                break

    # version - look for "Version" followed by number or "v1.2"
    version = ""
    m = re.search(r"Version[:\s]*([0-9A-Za-z\.\-v]+)", text, flags=re.IGNORECASE)
    if m:
        version = m.group(1).strip()

    # date - look for YYYY-MM-DD or common formats like MM/DD/YYYY or Month DD, YYYY -> normalize best-effort
    date = ""
    m = re.search(r"(\d{4}-\d{2}-\d{2})", text)
    if not m:
        m = re.search(r"(\d{2}/\d{2}/\d{4})", text)
    if not m:
        m = re.search(r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},\s+\d{4}", text, flags=re.IGNORECASE)
    if m:
        date = m.group(1)

    # keywords: pick most frequent short words excluding stopwords and numbers (simple)
    words = re.findall(r"\b[a-zA-Z]{3,20}\b", text.lower())
    stop = set(["the","and","for","with","that","this","from","will","are","not","have","has","was","were","they","their","which","would","should","can","may","also","all","any"])
    freq = {}
    for w in words:
        if w in stop:
            continue
        freq[w] = freq.get(w,0)+1
    sorted_kws = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    keywords = [k for k,_ in sorted_kws[:3]]

    # semantic topic & business domain: cheap heuristics by keyword mapping
    domain = ""
    semantic_topic = ""
    if any(k in text.lower() for k in ["access","production","prod","privilege","entitlement","approval"]):
        domain = "IAM"
        semantic_topic = "access control"
    elif any(k in text.lower() for k in ["audit","reconcile","reconciliation","report","revision"]):
        domain = "Compliance"
        semantic_topic = "report reconciliation"
    elif any(k in text.lower() for k in ["security","vulnerability","config"]):
        domain = "Security"
        semantic_topic = "security configuration"
    else:
        domain = "PCM"
        semantic_topic = "procedure"

    # risk heuristic
    risk = "low"
    if any(k in text.lower() for k in ["production","prod","privilege","admin","root","escalat"]):
        risk = "high"
    elif any(k in text.lower() for k in ["ticket","review","audit","compliance"]):
        risk = "medium"

    # summary: first 18-25 words from the first paragraph
    summary = ""
    for p in text.split("\n\n"):
        s = p.strip()
        if s:
            tokens = s.split()
            summary = " ".join(tokens[:25])
            break

    doc.metadata.setdefault("section_title", title or "")
    doc.metadata.setdefault("document_version", version or "")
    doc.metadata.setdefault("effective_date", date or "")
    doc.metadata.setdefault("semantic_topic", semantic_topic or "uncategorized")
    doc.metadata.setdefault("business_domain", domain or "")
    doc.metadata.setdefault("risk_level", risk)
    doc.metadata.setdefault("keywords", keywords)
    doc.metadata.setdefault("summary", summary or "")

# -------------------------
# ENRICH ALL CHUNKS
# -------------------------
def enrich_all(chunks: List[Document]):
    for i, c in enumerate(chunks, 1):
        llm_metadata_enrich(c)
    print("[✓] Enrichment complete")

# -------------------------
# VECTORSTORE BUILD + SAVE
# -------------------------
def build_vectorstores(chunksA: List[Document], chunksB: List[Document]):
    os.makedirs(VECTOR_DIR, exist_ok=True)
    vA = FAISS.from_documents(chunksA, embeddings)
    vB = FAISS.from_documents(chunksB, embeddings)
    vA.save_local(os.path.join(VECTOR_DIR, "vsA"))
    vB.save_local(os.path.join(VECTOR_DIR, "vsB"))
    print("[✓] Built & saved vectorstores vsA and vsB")
    return vA, vB

def load_vectorstores():
    vA = FAISS.load_local(os.path.join(VECTOR_DIR, "vsA"), embeddings, allow_dangerous_deserialization=True)
    vB = FAISS.load_local(os.path.join(VECTOR_DIR, "vsB"), embeddings, allow_dangerous_deserialization=True)
    print("[✓] Loaded vectorstores")
    return vA, vB

# -------------------------
# QUERY REWRITE (simple wrapper)
# -------------------------
def rewrite_query(q: str) -> str:
    resp = llm.invoke([
        {"role":"system","content":"Rewrite the query into a concise search query. Output only the rewritten query."},
        {"role":"user","content": REWRITE_PROMPT if False else q}
    ])
    return resp.content.strip() if hasattr(resp, "content") else str(resp).strip()

# -------------------------
# RETRIEVE FROM BOTH (similarity + mmr hybrid)
# -------------------------
def retrieve_from_both(query: str, vA: FAISS, vB: FAISS, k=TOP_K):
    q2 = rewrite_query(query) or query
    # similarity
    rA_sim = vA.similarity_search(q2, k=k)
    rA_mmr = vA.max_marginal_relevance_search(q2, k=k, fetch_k=2*k, lambda_mult=0.7)
    rB_sim = vB.similarity_search(q2, k=k)
    rB_mmr = vB.max_marginal_relevance_search(q2, k=k, fetch_k=2*k, lambda_mult=0.7)
    combined = rA_sim + rA_mmr + rB_sim + rB_mmr
    uniq = []
    seen = set()
    for d in combined:
        txt = (d.page_content or "").strip()
        if txt and txt not in seen:
            uniq.append(d)
            seen.add(txt)
    print(f"[+] Retrieved unique chunks: {len(uniq)}")
    return uniq[:2*k]

# -------------------------
# BUILD ANSWER (LLM uses provided context only)
# -------------------------
def build_answer(context_docs: List[Document], query: str) -> str:
    ctx = []
    for i,d in enumerate(context_docs,1):
        meta = d.metadata
        header = f"[{i}] source={meta.get('source_file','')} section_title={meta.get('section_title','')}"
        ctx.append(header + "\n" + d.page_content[:2000])  # limit chunk displayed size
    ctx_str = "\n\n---\n\n".join(ctx)
    resp = llm.invoke([
        {"role":"system","content": ANSWER_PROMPT},
        {"role":"user","content": f"Context:\n{ctx_str}\n\nQuestion:\n{query}"}
    ])
    return resp.content if hasattr(resp,"content") else str(resp)

# -------------------------
# ask(query) API
# -------------------------
def ask(query: str) -> str:
    vA, vB = load_vectorstores()
    docs = retrieve_from_both(query, vA, vB, k=TOP_K)
    return build_answer(docs, query)

# -------------------------
# MAIN pipeline (execute to create vectorstores)
# -------------------------
if __name__ == "__main__":
    # 1. Load raw docs
    raw_docs = load_raw_docs(DOCUMENTS_DIR)

    # 2. Create structural chunks (A)
    chunksA = structural_chunks(raw_docs)

    # 3. Create agentic chunks (B)
    chunksB = agentic_chunks(raw_docs)

    # 4. Enrich both sets (LLM + fallback)
    enrich_all(chunksA)
    enrich_all(chunksB)

    # 5. Save chunk dumps for inspection
    os.makedirs("out_chunks", exist_ok=True)
    def dump_chunks(chunks, name):
        arr=[]
        for c in chunks:
            arr.append({"page_content": c.page_content, "metadata": c.metadata})
        with open(f"out_chunks/{name}.json","w",encoding="utf-8") as f:
            json.dump(arr, f, ensure_ascii=False, indent=2)
    dump_chunks(chunksA, "chunksA_enriched")
    dump_chunks(chunksB, "chunksB_enriched")

    # 6. Build vectorstores and save
    vA, vB = build_vectorstores(chunksA, chunksB)

    # quick test
    print(ask("What is the production access policy?"))
