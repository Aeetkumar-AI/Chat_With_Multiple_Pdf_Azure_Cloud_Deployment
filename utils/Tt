"""
FULL ENTERPRISE DOCUMENT INGEST + METADATA + SUMMARIZATION + RAG PIPELINE
Supports: PDF, DOCX, TXT, MD
Designed for: GRC / PCM / SOP / Compliance / Audit
"""

############################
# LIBRARIES
############################
import os, json, glob
from typing import List, Dict
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.messages import SystemMessage, HumanMessage

########################################
# ENV + MODEL
########################################
load_dotenv()

# ðŸ‘‰ replace with your model init
from api.model import initialize_llm
llm = initialize_llm()

# ðŸ‘‰ replace with your embeddings
from vertex_langchain import VertexClientEmbeddings
emb = VertexClientEmbeddings()

DOCS_DIR = "Docs"
CHUNK_SIZE = 900
CHUNK_OVERLAP = 150

########################################
# ðŸ”¥ Step 1 - Load all file formats
########################################
def load_all_docs() -> List[Document]:
    docs = []

    for file in glob.glob(os.path.join(DOCS_DIR, "*")):
        ext = file.lower()

        if ext.endswith(".md"):
            loader = UnstructuredMarkdownLoader(file)
        elif ext.endswith(".pdf"):
            loader = PyPDFLoader(file)
        elif ext.endswith(".docx"):
            loader = Docx2txtLoader(file)
        elif ext.endswith(".txt"):
            loader = TextLoader(file, encoding="utf-8")
        else:
            continue

        for d in loader.load():
            d.metadata["source_file"] = os.path.basename(file)
            docs.append(d)

    print(f"[+] Loaded {len(docs)} documents from {DOCS_DIR}")
    return docs


raw_docs = load_all_docs()


########################################
# ðŸ”¥ Step 2 â€” Hierarchical Markdown Split
########################################
def hierarchical_md_split(docs: List[Document]) -> List[Document]:
    splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=[
            ("#", "H1"),
            ("##", "H2"),
            ("###", "H3"),
            ("####", "H4"),
        ],
        strip_headers=False,
    )

    final = []
    for d in docs:
        try:
            parts = splitter.split_text(d.page_content)
            for p in parts:
                p.metadata["source_file"] = d.metadata["source_file"]
                final.append(p)
        except:
            final.append(d)

    print(f"[âœ“] Markdown split â†’ {len(final)} sections")
    return final


stage1_docs = hierarchical_md_split(raw_docs)

########################################
# ðŸ”¥ Step 3 â€” Recursive Chunking
########################################
def recursive_chunk(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " "],
        length_function=len,
    )

    chunks = []
    for d in docs:
        sub = splitter.split_documents([d])
        for c in sub:
            c.metadata["source_file"] = d.metadata["source_file"]
            chunks.append(c)

    print(f"[âœ“] Recursive chunking â†’ {len(chunks)} chunks")
    return chunks


stage2_chunks = recursive_chunk(stage1_docs)

########################################
# ðŸ”¥ Step 4 â€” Agentic Chunking (LLM safe)
########################################

AGENTIC_PROMPT = """
You are an enterprise document chunking agent.
Document Types: Compliance Manuals, PCM, SOP, Audit Reports.

RULES:
- Preserve original text
- Do NOT rewrite
- Keep tables, code, lists intact
- Never split mid-sentence
- Keep headings with first paragraphs
- Keep procedures together
- Keep image references with section

CHUNK TARGET:
300â€“600 words
Min 100, Max flexible

Return chunks separated by:
===CHUNK===
"""

def agentic_chunk(text):
    r = llm.invoke([
        SystemMessage(content=AGENTIC_PROMPT),
        HumanMessage(content=text)
    ])
    return [c.strip() for c in r.content.split("===CHUNK===") if c.strip()]


def agentic_stage(chunks: List[Document]):
    final = []
    for d in chunks:
        parts = agentic_chunk(d.page_content)
        for i, p in enumerate(parts):
            final.append(Document(
                page_content=p,
                metadata={
                    "source_file": d.metadata["source_file"],
                    "chunk_type": "agentic",
                    "chunk_index": i
                }
            ))
    print(f"[âœ“] Agentic chunks: {len(final)}")
    return final


chunksA = stage2_chunks           # classical split
chunksB = agentic_stage(stage2_chunks)  # LLM split

########################################
# ðŸ”¥ Step 5 â€” LLM Metadata Enrichment
########################################
META_PROMPT = """
You are a metadata extraction AI.
Extract fields as JSON ONLY:

{
"document_version": string | null,
"effective_date": string | null,
"business_domain": string | null,
"semantic_topic": string,
"keywords": [string, ...]  # 6â€“8 max
}

RULES:
- Do NOT invent
- Base ONLY on text
- If missing â†’ null or empty list
- Identify product/system names as keywords
"""

def enrich_metadata(doc: Document):
    sample = doc.page_content[:1500]

    r = llm.invoke([
        SystemMessage(content=META_PROMPT),
        HumanMessage(content=sample)
    ])
    try:
        meta = json.loads(r.content)
        doc.metadata.update(meta)
    except:
        doc.metadata["semantic_topic"] = "uncategorized"
        doc.metadata["keywords"] = []


for d in chunksB:
    enrich_metadata(d)

print("[âœ“] Metadata enrichment done")


########################################
# ðŸ”¥ Step 6 â€” Document Summarization
########################################

SUMM_PROMPT = """
You are an expert auditor and compliance analyst.

Summarize content STRICTLY according to document type.
Allowed types:
- PCM / Compliance SOP
- Business process
- Audit / Review
- IAM / Access

OUTPUT FORMAT:

- Purpose:
- Scope:
- Roles:
- Steps (bullet):
- Control points:
- Risks:
- Exceptions:
- Document Version:
- Effective Date:

RULES:
- No creativity
- No assumptions
- No rewriting procedures
- If missing â†’ write "Not mentioned"
"""

def summarize_doc(text: str):
    r = llm.invoke([
        SystemMessage(content=SUMM_PROMPT),
        HumanMessage(content=text)
    ])
    return r.content


########################################
# ðŸ”¥ Step 7 VC â†’ Build Vector Stores
########################################
VS_A = FAISS.from_documents(chunksA, emb)
VS_B = FAISS.from_documents(chunksB, emb)

VS_A.save_local("vs_A")
VS_B.save_local("vs_B")

########################################
# ðŸ”¥ Step 8 â€” Retrieval + Query Rewriting
########################################

QUERY_REWRITE = """
Rewrite the user question to a precise enterprise compliance search query.
Return ONLY rewritten query.
Avoid long sentences.
"""

def rewrite_query(q):
    r = llm.invoke([
        SystemMessage(content=QUERY_REWRITE),
        HumanMessage(content=q)
    ])
    return r.content.strip()

def retrieve(query, k=10):
    q2 = rewrite_query(query)

    a = VS_A.similarity_search(q2, k=k)
    b = VS_B.similarity_search(q2, k=k)

    combined = a + b
    seen, uniq = set(), []
    for c in combined:
        t = c.page_content.strip()
        if t not in seen:
            seen.add(t)
            uniq.append(c)
    return uniq[:k*2]


########################################
# ðŸ”¥ Step 9 â€” Generate Answer
########################################
ANSWER_PROMPT = """
You are GRC/PCM analyst.
Use ONLY the provided context.
Never invent. If missing â†’ say missing.

Answer format:
- Explanation
- Procedures
- Control steps
- Version & Date if present
"""

def build_answer(context_docs, query):
    joined = "\n\n".join([d.page_content for d in context_docs])
    r = llm.invoke([
        SystemMessage(content=ANSWER_PROMPT),
        HumanMessage(content=f"CONTEXT:\n{joined}"),
        HumanMessage(content=f"QUESTION:\n{query}")
    ])
    return r.content


def ask(query):
    ctx = retrieve(query)
    return build_answer(ctx, query)
