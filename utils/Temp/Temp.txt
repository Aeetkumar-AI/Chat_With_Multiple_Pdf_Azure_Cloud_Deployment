import os, glob, json
from typing import List, Dict
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage

# -------------------- CONFIG --------------------
DOC_PATH = "Doc"                   # folder with .md files
MODEL_NAME = "gemini-2.5-pro"      # or gemini-1.5-pro
CHUNKS_OUTPUT = "chunked_docs.json"
TEMPERATURE = 0
MAX_TOKENS = 8192

# -------------------- LLM --------------------
AGENTIC_PROMPT = """
You are an expert document–chunking assistant specializing in PCM, compliance documents, SOP manuals, and technical documentation.

================ CORE OBJECTIVE ================
Split the document into semantic chunks that:
1. Maintain complete meaning (never orphan text)
2. Preserve ALL formatting (tables, lists, code, metadata)
3. Respect natural boundaries (sections, workflows, procedures)
4. Optimize for vector databases (FAISS/Chroma/Pinecone)
5. Improve retrieval accuracy for RAG systems.

================ STRICT RULES ================
⚠ NEVER SPLIT:
- Metadata blocks
- Table of Contents
- Tables (any size)
- Bullet/numbered lists
- Step-by-step procedures
- Code blocks
- Diagrams
- Revision history
- Headings + their first paragraph
- Images + captions
- URLs / UNC paths with context

⚠ PRESERVE EXACT FORMATTING:
- Keep markdown headings
- Keep tables intact
- Keep indentation + spacing
- DO NOT rewrite text

SEMANTIC BOUNDARIES:
- One complete idea per chunk
- Never split mid-sentence
- Never break logically linked paragraphs

SIZE GUIDELINE:
- Target 300–600 words
- Min 100 (metadata allowed smaller)
- Larger allowed if splitting destroys meaning

NESTED STRUCTURES → SINGLE CHUNK:
- Table with bullets
- List with sub-tables
- Procedure + diagram

CROSS REFERENCES:
Always keep references with topic:
- “See section 4.3”
- Hyperlinks
- UNC paths

IMAGES:
Keep referenced images in same chunk.

================ OUTPUT FORMAT ================
Return ONLY valid JSON array:
[
  {
    "chunk_id": 1,
    "content": "Text here with preserved formatting..."
  }
]
"""

llm = ChatGoogleGenerativeAI(
    model=MODEL_NAME,
    temperature=TEMPERATURE,
    max_output_tokens=MAX_TOKENS,
)

# -------------------- LOAD MARKDOWN DOCS --------------------
def load_docs(path: str) -> List[Dict]:
    docs = []
    for file in glob.glob(os.path.join(path, "*.md")):
        loader = UnstructuredMarkdownLoader(file)
        for d in loader.load():
            d.metadata["source"] = os.path.basename(file)
            docs.append(d)
    print(f"[+] Loaded {len(docs)} markdown docs from {path}")
    return docs

# -------------------- LLM AGENTIC CHUNKING --------------------
def agentic_chunk(text: str):
    resp = llm.invoke([
        HumanMessage(
            content=f"{AGENTIC_PROMPT}\n\n=== DOCUMENT START ===\n{text}\n=== DOCUMENT END ==="
        )
    ])
    return resp.content

# -------------------- MAIN PIPELINE --------------------
def chunk_documents_agentic(docs: List) -> List[Dict]:
    all_chunks = []
    for idx, doc in enumerate(docs, 1):
        print(f"\n[+] Chunking doc {idx}/{len(docs)}: {doc.metadata['source']}")

        try:
            result = agentic_chunk(doc.page_content)
            parsed = json.loads(result)
        except Exception as e:
            print(f"[!] ERROR parsing JSON → {e}")
            continue

        for item in parsed:
            all_chunks.append({
                "source": doc.metadata["source"],
                "chunk_id": item["chunk_id"],
                "content": item["content"].strip()
            })

    print(f"\n[✓] Total Chunks Created: {len(all_chunks)}")
    return all_chunks

# -------------------- SAVE OUTPUT JSON --------------------
def save_chunks(chunks: List[Dict], out_file: str):
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    print(f"[✓] Saved chunks to {out_file}")

# -------------------- RUN --------------------
if __name__ == "__main__":
    docs = load_docs(DOC_PATH)
    chunks = chunk_documents_agentic(docs)
    save_chunks(chunks, CHUNKS_OUTPUT)
