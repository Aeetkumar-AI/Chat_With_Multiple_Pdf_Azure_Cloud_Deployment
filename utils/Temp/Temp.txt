"""
=== FULL PIPELINE ONE PAGE ===
- Load markdown documents
- Normal Chunking
- Agentic Chunking (LLM)
- Save chunks locally (JSON)
- Build FAISS A + B
- Save FAISS locally
- Retrieve from both
- Answer using Gemini
"""

import os
import json
import glob
from typing import List
from langchain.schema import Document
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

# ====================================================================
# CONFIG
# ====================================================================

DOCUMENTS_DIR = "Doc"
CHUNK_SAVE_A = "chunksA.json"
CHUNK_SAVE_B = "chunksB.json"
VS_DIR_A = "vsA"
VS_DIR_B = "vsB"
TOP_K = 6

os.environ["GOOGLE_API_KEY"] = "YOUR_GEMINI_API_KEY"

embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")

# ====================================================================
# 1. Load Docs
# ====================================================================

def load_docs_md(path: str) -> List[Document]:
    docs = []
    for f in glob.glob(os.path.join(path, "*.md")):
        loader = UnstructuredMarkdownLoader(f)
        docs.extend(loader.load())
    print(f"[+] Loaded {len(docs)} documents")
    return docs

# ====================================================================
# 2. Normal Chunking
# ====================================================================

def chunk_docs_normal(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)
    chunks = splitter.split_documents(docs)
    print(f"[+] Normal chunks: {len(chunks)}")
    return chunks

# ====================================================================
# 3. Agentic Chunking (LLM → returns list of Document)
# ====================================================================

def agentic_chunk(doc: Document) -> List[Document]:
    prompt = f"""
You are an expert document chunking assistant.
Split the document into semantically meaningful sections.
Return each chunk clearly separated using this delimiter:

<CHUNK>
...text...
</CHUNK>

Document:
{doc.page_content}
"""

    res = llm.invoke(prompt).content
    parts = res.split("<CHUNK>")
    chunks = []

    for p in parts:
        p = p.replace("</CHUNK>", "").strip()
        if len(p) > 50:
            chunks.append(Document(
                page_content=p,
                metadata={"source": doc.metadata.get("source", "unknown")}
            ))

    return chunks


def chunk_docs_agentic(docs: List[Document]) -> List[Document]:
    all_chunks = []
    for i, d in enumerate(docs, 1):
        print(f"[↓] Agentic chunk doc {i}/{len(docs)}: {d.metadata.get('source')}")
        try:
            parts = agentic_chunk(d)
            all_chunks.extend(parts)
        except Exception as e:
            print("[!] Agentic error =>", e)

    print(f"[+] Agentic chunks: {len(all_chunks)}")
    return all_chunks

# ====================================================================
# 4. SAVE / LOAD CHUNKS LOCALLY
# ====================================================================

def save_chunks_json(chunks, path):
    data = [{"page_content": d.page_content, "metadata": d.metadata} for d in chunks]
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"[+] Saved {len(chunks)} to {path}")


def load_chunks_json(path) -> List[Document]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    docs = [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in data]
    print(f"[+] Loaded {len(docs)} chunks from {path}")
    return docs

# ====================================================================
# 5. Build FAISS A + B
# ====================================================================

def build_vectorstores(chunksA, chunksB):
    vsA = FAISS.from_documents(chunksA, embeddings)
    vsB = FAISS.from_documents(chunksB, embeddings)
    print("[+] Built VS A & VS B")
    return vsA, vsB


def save_vectorstores(vsA, vsB):
    vsA.save_local(VS_DIR_A)
    vsB.save_local(VS_DIR_B)
    print("[+] Vectorstores saved")


def load_vectorstores():
    vsA = FAISS.load_local(VS_DIR_A, embeddings, allow_dangerous_deserialization=True)
    vsB = FAISS.load_local(VS_DIR_B, embeddings, allow_dangerous_deserialization=True)
    print("[+] Vectorstores loaded")
    return vsA, vsB

# ====================================================================
# 6. Retrieve FROM BOTH (NO reranker)
# ====================================================================

def retrieve_from_both(query: str, vsA, vsB):
    rA = vsA.similarity_search(query, k=TOP_K)
    rB = vsB.similarity_search(query, k=TOP_K)

    combined = rA + rB

    uniq, seen = [], set()
    for d in combined:
        txt = d.page_content.strip()
        if txt not in seen:
            seen.add(txt)
            uniq.append(d)

    print(f"[+] Combined unique retrieved: {len(uniq)}")
    return uniq[:TOP_K * 2]

# ====================================================================
# 7. Ask LLM
# ====================================================================

def ask_llm(query, contexts: List[Document]):
    ctx = "\n\n---\n\n".join([d.page_content for d in contexts])
    prompt = f"""
You are an expert.
Use ONLY this context to answer:

Context:
{ctx}

User query: {query}
Return a clear and correct answer.
"""
    return llm.invoke(prompt).content

# ====================================================================
# RUN FIRST TIME
# ====================================================================

if __name__ == "__main__":

    raw_docs = load_docs_md(DOCUMENTS_DIR)

    # CHUNKING
    chunksA = chunk_docs_normal(raw_docs)
    chunksB = chunk_docs_agentic(raw_docs)

    # SAVE locally
    save_chunks_json(chunksA, CHUNK_SAVE_A)
    save_chunks_json(chunksB, CHUNK_SAVE_B)

    # BUILD FAISS
    vsA, vsB = build_vectorstores(chunksA, chunksB)
    save_vectorstores(vsA, vsB)

    print("\nPipeline Completed ✔")

    # =========================
    # DEMO QUERY
    # =========================
    question = "What is production access policy?"
    ctx = retrieve_from_both(question, vsA, vsB)
    answer = ask_llm(question, ctx)
    print("\n=== ANSWER ===")
    print(answer)
