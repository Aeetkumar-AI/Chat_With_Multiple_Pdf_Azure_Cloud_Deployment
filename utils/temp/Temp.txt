import google.generativeai as genai
from langchain.retrievers import BM25Retriever

# SET API KEY
genai.configure(api_key="YOUR_GEMINI_API_KEY")


########################################
# RETRIEVAL HELPERS
########################################

def retrieve_all(vectorstore, chunked_docs, query, k=10):
    # --- FAISS Similarity Search ---
    sim_docs = vectorstore.similarity_search(query, k=k)

    # --- FAISS MMR (diversity) ---
    mmr_docs = vectorstore.max_marginal_relevance_search(
        query,
        k=k,
        fetch_k=k * 2,
        lambda_mult=0.7
    )

    # --- BM25 Keyword Search ---
    bm25_ret = BM25Retriever.from_documents(chunked_docs)
    bm25_ret.k = k
    bm25_docs = bm25_ret.get_relevant_documents(query)

    # --- Dense direct ---
    dense_docs = vectorstore.similarity_search_with_score(query, k=k)
    dense_docs = [d[0] for d in dense_docs]

    return sim_docs + mmr_docs + bm25_docs + dense_docs


def dedupe_docs(docs):
    seen = set()
    unique = []
    for d in docs:
        key = (d.page_content.strip(), str(d.metadata))
        if key not in seen:
            seen.add(key)
            unique.append(d)
    return unique


########################################
# PROMPT + LLM
########################################

def build_prompt(query, docs):
    context = "\n\n".join(d.page_content for d in docs)
    return f"""
You are an expert assistant. Answer strictly based on the context.

### CONTEXT
{context}

### QUESTION
{query}

RULES:
- Only answer using context
- If context does not contain enough info reply "Insufficient information"
- Be precise and factual
"""


def llm_answer(prompt, model="models/gemini-2.0-pro-exp-01"):
    model = genai.GenerativeModel(model)
    response = model.generate_content(prompt)
    return response.text


########################################
# MAIN â€” USER ONLY ENTERS QUERY
########################################

def ask(query, k=10):
    raw_docs = retrieve_all(vectorstore, chunked_docs, query, k)
    unique_docs = dedupe_docs(raw_docs)
    prompt = build_prompt(query, unique_docs[:k])
    return llm_answer(prompt)
