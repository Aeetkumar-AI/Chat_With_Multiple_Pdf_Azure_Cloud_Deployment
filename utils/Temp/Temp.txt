import os, glob
from typing import List
from langchain.schema import Document
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, SystemMessage


AGENTIC_PROMPT = """
You are an expert document-chunking assistant.

=== OBJECTIVE ===
Split the document into logical chunks preserving:
- meaning
- formatting
- markdown tables
- lists
- procedures
- code
- headings

Chunk size target: 300–600 words.
Never split tables, procedures, lists, or code blocks.

Return chunks separated by:
===CHUNK===
Do NOT explain anything.
Do NOT use JSON.
Only output raw chunks.
"""

MODEL_NAME = "gemini-2.5-pro"
DOC_PATH = "Doc"
TEMPERATURE = 0
MAX_TOKENS = 8192


llm = ChatGoogleGenerativeAI(
    model=MODEL_NAME,
    temperature=TEMPERATURE,
    max_output_tokens=MAX_TOKENS,
)


# ------------------ LOAD MARKDOWN ------------------
def load_docs(path: str) -> List[Document]:
    docs = []
    for file in glob.glob(os.path.join(path, "*.md")):
        loader = UnstructuredMarkdownLoader(file)
        for d in loader.load():
            d.metadata["source"] = os.path.basename(file)
            docs.append(d)

    print(f"[+] Loaded {len(docs)} markdown docs from {path}")
    return docs


# ------------------ CHUNK USING LLM ------------------
def agentic_chunk(text: str) -> List[str]:
    resp = llm.invoke([
        SystemMessage(content=AGENTIC_PROMPT),
        HumanMessage(content=text)
    ])
    raw = resp.content
    # split output into chunks
    # each chunk separated by delimiter
    chunks = [c.strip() for c in raw.split("===CHUNK===") if c.strip()]
    return chunks


# ------------------ CONVERT TO DOCUMENT OBJECTS ------------------
def chunk_documents_agentic(docs: List[Document]) -> List[Document]:
    all_chunks = []

    for idx, doc in enumerate(docs, 1):
        print(f"[+] Chunking {idx}/{len(docs)} → {doc.metadata['source']}")
        
        chunks = agentic_chunk(doc.page_content)
        for i, c in enumerate(chunks, 1):
            all_chunks.append(
                Document(
                    page_content=c,
                    metadata={
                        "source": doc.metadata["source"],
                        "chunk_index": i
                    }
                )
            )

    print(f"[✓] Created {len(all_chunks)} LLM-chunks")
    return all_chunks


# ------------------ MAIN ------------------
if __name__ == "__main__":
    raw_docs = load_docs(DOC_PATH)
    chunks = chunk_documents_agentic(raw_docs)
    print("\nExample Chunk:", chunks[0].page_content[:300])
