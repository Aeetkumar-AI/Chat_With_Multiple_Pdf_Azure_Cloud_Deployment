# ============================================
# DOCUMENT LOADING UTILITIES
# ============================================

class DocumentLoader:
    """
    Handles loading of different document types (PDF, DOCX, MD, TXT)
    """
    
    @staticmethod
    def load_pdf(file_path: str) -> str:
        """
        Extract text from PDF file
        
        Args:
            file_path: Path to PDF file
            
        Returns:
            str: Extracted text content
        """
        text = ""
        try:
            # Open PDF file in binary read mode
            with open(file_path, 'rb') as file:
                # Create PDF reader object
                pdf_reader = PyPDF2.PdfReader(file)
                
                # Iterate through all pages
                for page_num, page in enumerate(pdf_reader.pages):
                    # Extract text from each page
                    page_text = page.extract_text()
                    text += page_text + "\n\n"
                    
            print(f"  âœ“ Loaded PDF: {os.path.basename(file_path)} ({len(pdf_reader.pages)} pages)")
            
        except Exception as e:
            print(f"  âœ— Error loading PDF {file_path}: {str(e)}")
            
        return text
    
    @staticmethod
    def load_docx(file_path: str) -> str:
        """
        Extract text from DOCX file
        
        Args:
            file_path: Path to DOCX file
            
        Returns:
            str: Extracted text content
        """
        text = ""
        try:
            # Load DOCX document
            doc = Document(file_path)
            
            # Extract text from all paragraphs
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            # Extract text from tables
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        text += cell.text + "\t"
                    text += "\n"
            
            print(f"  âœ“ Loaded DOCX: {os.path.basename(file_path)} ({len(doc.paragraphs)} paragraphs)")
            
        except Exception as e:
            print(f"  âœ— Error loading DOCX {file_path}: {str(e)}")
            
        return text
    
    @staticmethod
    def load_markdown(file_path: str) -> str:
        """
        Load markdown file
        
        Args:
            file_path: Path to markdown file
            
        Returns:
            str: File content
        """
        try:
            # Read markdown file as plain text
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
                
            print(f"  âœ“ Loaded Markdown: {os.path.basename(file_path)}")
            return text
            
        except Exception as e:
            print(f"  âœ— Error loading Markdown {file_path}: {str(e)}")
            return ""
    
    @staticmethod
    def load_text(file_path: str) -> str:
        """
        Load plain text file
        
        Args:
            file_path: Path to text file
            
        Returns:
            str: File content
        """
        try:
            # Read text file with UTF-8 encoding
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
                
            print(f"  âœ“ Loaded Text: {os.path.basename(file_path)}")
            return text
            
        except Exception as e:
            print(f"  âœ— Error loading Text {file_path}: {str(e)}")
            return ""
    
    @classmethod
    def load_document(cls, file_path: str) -> Tuple[str, str]:
        """
        Load document based on file extension
        
        Args:
            file_path: Path to document
            
        Returns:
            tuple: (content, file_type)
        """
        # Get file extension (lowercase)
        ext = os.path.splitext(file_path),[object Object],lower()
        
        # Route to appropriate loader based on extension
        if ext == '.pdf':
            return cls.load_pdf(file_path), 'pdf'
        elif ext == '.docx':
            return cls.load_docx(file_path), 'docx'
        elif ext == '.md':
            return cls.load_markdown(file_path), 'markdown'
        elif ext == '.txt':
            return cls.load_text(file_path), 'text'
        else:
            print(f"  âš  Unsupported file type: {ext}")
            return "", "unknown"

print("âœ… DocumentLoader class defined successfully!")


# ============================================
# DOCUMENT PREPROCESSING
# ============================================

class DocumentPreprocessor:
    """
    Cleans and preprocesses document content before chunking
    """
    
    @staticmethod
    def extract_base64_images(content: str) -> Tuple[str, Dict]:
        """
        Extract base64-encoded images and replace with references
        
        This prevents massive base64 strings from cluttering chunks
        
        Args:
            content: Document content with embedded images
            
        Returns:
            tuple: (cleaned_content, image_dictionary)
        """
        
        # Regular expression to match base64-encoded images in markdown
        # Pattern breakdown:
        # !\[([^\]]*)\]           - Match ![alt text]
        # \(data:image/          - Match (data:image/
        # ([^;]+)                - Capture image type (png, jpg, etc.)
        # ;base64,               - Match ;base64,
        # ([A-Za-z0-9+/=]+)      - Capture base64 data (letters, numbers, +, /, =)
        # \)                     - Match closing )
        
        image_pattern = r'!\[([^\]]*)\]\(data:image/([^;]+);base64,([A-Za-z0-9+/=]+)\)'
        
        images = {}  # Dictionary to store extracted images
        
        def replace_image(match):
            """
            Replacement function for re.sub()
            Extracts image data and creates a reference
            """
            alt_text = match.group(1)      # Image alt text
            img_type = match.group(2)      # Image type (png, jpg, etc.)
            base64_data = match.group(3)   # Base64 encoded data
            
            # Create unique hash for this image (using first 100 chars of base64)
            img_hash = hashlib.md5(base64_data[:100].encode()).hexdigest()[:8]
            
            # Store image data in dictionary
            images[img_hash] = {
                'alt_text': alt_text,
                'type': img_type,
                'data_preview': base64_data[:50] + '...',  # Store only preview
                'full_length': len(base64_data)
            }
            
            # Replace with simple reference
            return f'![{alt_text}](IMAGE_REF_{img_hash})'
        
        # Replace all base64 images with references
        cleaned_content = re.sub(image_pattern, replace_image, content)
        
        if images:
            print(f"  â„¹ Extracted {len(images)} base64-encoded images")
        
        return cleaned_content, images
    
    @staticmethod
    def normalize_whitespace(content: str) -> str:
        """
        Normalize excessive whitespace and line breaks
        
        Args:
            content: Document content
            
        Returns:
            str: Content with normalized whitespace
        """
        
        # Replace Windows line endings (\r\n) with Unix line endings (\n)
        content = content.replace('\r\n', '\n')
        
        # Replace multiple consecutive newlines with maximum 2 newlines
        # Pattern: \n{3,} matches 3 or more consecutive newlines
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # Remove trailing whitespace from each line
        lines = content.split('\n')
        content = '\n'.join(line.rstrip() for line in lines)
        
        return content
    
    @staticmethod
    def fix_unc_paths(content: str) -> str:
        """
        Fix UNC paths with excessive backslashes
        
        UNC paths like \\\\server\\share can become \\\\\\\\\\server\\\\\\share
        This normalizes them to proper format
        
        Args:
            content: Document content
            
        Returns:
            str: Content with fixed UNC paths
        """
        
        # Replace 5 or more consecutive backslashes with exactly 4 (\\\\)
        # Pattern: \\{5,} matches 5 or more backslashes
        content = re.sub(r'\\{5,}', r'\\\\', content)
        
        return content
    
    @staticmethod
    def remove_control_characters(content: str) -> str:
        """
        Remove problematic control characters that can break JSON
        
        Args:
            content: Document content
            
        Returns:
            str: Content with control characters removed
        """
        
        # Keep only characters with ASCII value >= 32 (printable)
        # Plus newline (\n), tab (\t), and carriage return (\r)
        content = ''.join(
            char for char in content 
            if ord(char) >= 32 or char in '\n\t\r'
        )
        
        return content
    
    @classmethod
    def preprocess(cls, content: str) -> Tuple[str, Dict]:
        """
        Complete preprocessing pipeline
        
        Args:
            content: Raw document content
            
        Returns:
            tuple: (preprocessed_content, metadata)
        """
        
        # Extract and replace base64 images
        content, images = cls.extract_base64_images(content)
        
        # Normalize whitespace
        content = cls.normalize_whitespace(content)
        
        # Fix UNC paths
        content = cls.fix_unc_paths(content)
        
        # Remove control characters
        content = cls.remove_control_characters(content)
        
        metadata = {
            'images_extracted': len(images),
            'images': images
        }
        
        return content, metadata

print("âœ… DocumentPreprocessor class defined successfully!")


# ============================================
# SEMANTIC CHUNKING - PART 1
# ============================================

class SemanticChunker:
    """
    Intelligently splits documents into semantic chunks
    Preserves tables, lists, procedures, and other structural elements
    """
    
    def __init__(self, 
                 min_chunk_size: int = 100,
                 target_chunk_size: int = 500,
                 max_chunk_size: int = 1000):
        """
        Initialize chunker with size parameters
        
        Args:
            min_chunk_size: Minimum words per chunk
            target_chunk_size: Target words per chunk
            max_chunk_size: Maximum words per chunk (flexible)
        """
        self.min_chunk_size = min_chunk_size
        self.target_chunk_size = target_chunk_size
        self.max_chunk_size = max_chunk_size
    
    @staticmethod
    def count_words(text: str) -> int:
        """
        Count words in text
        
        Args:
            text: Input text
            
        Returns:
            int: Word count
        """
        return len(text.split())
    
    @staticmethod
    def escape_json(text: str) -> str:
        """
        Properly escape text for JSON serialization
        
        This is CRITICAL to prevent JSON parsing errors
        
        Args:
            text: Text to escape
            
        Returns:
            str: JSON-safe text
        """
        
        # Dictionary of characters that need escaping in JSON
        escape_dict = {
            '\\': '\\\\',   # Backslash must be escaped first!
            '"': '\\"',     # Double quotes
            '\n': '\\n',    # Newlines
            '\r': '\\r',    # Carriage returns
            '\t': '\\t',    # Tabs
            '\b': '\\b',    # Backspace
            '\f': '\\f'     # Form feed
        }
        
        # Apply all escapes
        for char, escaped in escape_dict.items():
            text = text.replace(char, escaped)
        
        return text
    
    @staticmethod
    def identify_section_type(content: str) -> str:
        """
        Identify the type of content section
        
        This helps with retrieval - we can prioritize certain section types
        
        Args:
            content: Section content
            
        Returns:
            str: Section type identifier
        """
        
        # Check for Table of Contents
        # Pattern: Looks for "TABLE OF CONTENTS" with optional asterisks
        if re.search(r'\*{0,2}TABLE OF CONTENTS\*{0,2}', content, re.IGNORECASE):
            return 'toc'
        
        # Check for metadata block
        # Pattern: Lines with "Key: Value" format
        # Example: "Version: 3.1\nOwner: John Smith"
        if re.search(r'^\*{0,2}[A-Z][^*\n]+\*{0,2}\s*\n(?:[A-Za-z\s]+:.*\n)+', content, re.MULTILINE):
            return 'metadata'
        
        # Check for tables
        # Pattern: Lines containing pipe characters (|)
        if re.search(r'\|[^\n]+\|', content):
            return 'table'
        
        # Check for headings
        # Pattern: Markdown headings (# to ######)
        if re.match(r'#{1,6}\s', content):
            return 'section'
        
        # Check for numbered procedures
        # Pattern: Lines starting with "1. ", "2. ", etc.
        if re.search(r'^\d+\.\s+', content, re.MULTILINE):
            return 'procedure'
        
        # Check for bullet lists
        # Pattern: Lines starting with -, *, or +
        if re.search(r'^[-*+]\s+', content, re.MULTILINE):
            return 'list'
        
        # Default to generic content
        return 'content'

print("âœ… SemanticChunker Part 1 (Helper Methods) defined successfully!")

# ============================================
# SEMANTIC CHUNKING - PART 2
# Pattern Detection Methods
# ============================================

class SemanticChunker(SemanticChunker):  # Extends previous definition
    """
    Continuation of SemanticChunker class
    Pattern detection methods for identifying document structures
    """
    
    @staticmethod
    def extract_complete_table(content: str, start_pos: int) -> Tuple[str, int]:
        """
        Extract a complete table from content starting at position
        
        Tables are identified by pipe characters (|)
        This ensures we never split a table across chunks
        
        Args:
            content: Full document content
            start_pos: Starting position of table
            
        Returns:
            tuple: (complete_table_text, length_of_table)
        """
        
        # Get content from start position onwards
        remaining_content = content[start_pos:]
        lines = remaining_content.split('\n')
        
        table_lines = []
        in_table = False
        
        for line in lines:
            # Check if line contains pipe character (table row)
            if '|' in line:
                in_table = True
                table_lines.append(line)
            elif in_table and line.strip() == '':
                # Empty line after table - end of table
                break
            elif in_table and not line.strip().startswith('|'):
                # Line doesn't start with pipe but table might continue
                # (could be wrapped content)
                if line.strip():
                    table_lines.append(line)
        
        # Join all table lines
        complete_table = '\n'.join(table_lines)
        
        return complete_table, len(complete_table)
    
    @staticmethod
    def is_complete_table(text: str) -> bool:
        """
        Check if text contains a complete table
        
        A complete table has:
        1. Header row (with pipes)
        2. Separator row (with dashes and pipes)
        3. At least one data row
        
        Args:
            text: Text to check
            
        Returns:
            bool: True if complete table
        """
        
        lines = text.split('\n')
        table_lines = [line for line in lines if '|' in line]
        
        # Need at least 3 lines for complete table
        # (header, separator, data)
        if len(table_lines) < 3:
            return False
        
        # Check if second line is separator (contains dashes)
        # Pattern: |---|---|---| or |:---|:---:|---:|
        separator_pattern = r'\|[\s:-]+\|'
        
        if len(table_lines) > 1:
            if re.search(separator_pattern, table_lines,[object Object],):
                return True
        
        return False
    
    @staticmethod
    def extract_complete_list(content: str, start_pos: int) -> Tuple[str, int]:
        """
        Extract a complete numbered or bulleted list
        
        Lists should never be split across chunks
        
        Args:
            content: Full document content
            start_pos: Starting position of list
            
        Returns:
            tuple: (complete_list_text, length_of_list)
        """
        
        remaining_content = content[start_pos:]
        lines = remaining_content.split('\n')
        
        list_lines = []
        in_list = False
        list_type = None  # 'numbered' or 'bulleted'
        
        for line in lines:
            # Check for numbered list item
            # Pattern: "1. ", "2. ", "10. ", etc.
            if re.match(r'^\d+\.\s+', line):
                in_list = True
                list_type = 'numbered'
                list_lines.append(line)
            
            # Check for bulleted list item
            # Pattern: "- ", "* ", "+ " at start of line
            elif re.match(r'^[-*+]\s+', line):
                in_list = True
                list_type = 'bulleted'
                list_lines.append(line)
            
            # Check for indented sub-items
            # Pattern: Lines starting with spaces/tabs
            elif in_list and re.match(r'^\s+', line) and line.strip():
                list_lines.append(line)
            
            # Empty line might separate list items
            elif in_list and line.strip() == '':
                list_lines.append(line)
            
            # Non-list content - end of list
            elif in_list:
                break
        
        complete_list = '\n'.join(list_lines)
        
        return complete_list, len(complete_list)
    
    @staticmethod
    def find_heading_positions(content: str) -> List[Dict]:
        """
        Find all markdown heading positions in content
        
        Headings are section boundaries for chunking
        
        Args:
            content: Document content
            
        Returns:
            list: List of dicts with heading info
        """
        
        headings = []
        
        # Pattern for markdown headings
        # Matches: # Heading, ## Heading, ### Heading, etc.
        # Group 1: The hash marks (# to ######)
        # Group 2: The heading text
        heading_pattern = r'^(#{1,6})\s+(.+)$'
        
        lines = content.split('\n')
        current_pos = 0
        
        for line_num, line in enumerate(lines):
            match = re.match(heading_pattern, line)
            
            if match:
                level = len(match.group(1))  # Count number of # symbols
                text = match.group(2).strip()
                
                headings.append({
                    'level': level,
                    'text': text,
                    'line_num': line_num,
                    'position': current_pos
                })
            
            # Update position counter (add line length + newline)
            current_pos += len(line) + 1
        
        return headings

print("âœ… SemanticChunker Part 2 (Pattern Detection) defined successfully!")

# ============================================
# SEMANTIC CHUNKING - PART 3
# Main Chunking Logic
# ============================================

class SemanticChunker(SemanticChunker):  # Further extends the class
    """
    Continuation of SemanticChunker class
    Main chunking logic
    """
    
    def split_by_headings(self, content: str) -> List[str]:
        """
        Split content by major headings (# and ##)
        
        This creates initial large sections that will be further processed
        
        Args:
            content: Document content
            
        Returns:
            list: List of section texts
        """
        
        # Split by level 1 and 2 headings
        # Pattern: Newline followed by # or ##
        # (?=...) is a lookahead - matches position before pattern
        sections = re.split(r'\n(?=#{1,2}\s)', content)
        
        # Filter out empty sections
        sections = [s.strip() for s in sections if s.strip()]
        
        return sections
    
    def chunk_section(self, section: str, source_file: str, chunk_id_start: int) -> List[Dict]:
        """
        Chunk a single section into appropriately sized chunks
        
        This is the core chunking logic that respects semantic boundaries
        
        Args:
            section: Section text to chunk
            source_file: Source filename for metadata
            chunk_id_start: Starting chunk ID
            
        Returns:
            list: List of chunk dictionaries
        """
        
        chunks = []
        chunk_id = chunk_id_start
        
        # Count words in section
        word_count = self.count_words(section)
        
        # If section is small enough, keep as single chunk
        if word_count <= self.max_chunk_size:
            
            # But check if it's too small
            if word_count < self.min_chunk_size:
                # Only create chunk if it has meaningful content
                if section.strip() and not re.match(r'^#{1,6}\s*$', section):
                    chunks.append(self._create_chunk(
                        chunk_id=chunk_id,
                        content=section,
                        source_file=source_file
                    ))
            else:
                chunks.append(self._create_chunk(
                    chunk_id=chunk_id,
                    content=section,
                    source_file=source_file
                ))
            
            return chunks
        
        # Section is too large - need to split by subsections
        # Split by level 3+ headings
        subsections = re.split(r'\n(?=#{3,6}\s)', section)
        
        current_chunk = ""
        
        for subsection in subsections:
            subsection = subsection.strip()
            
            if not subsection:
                continue
            
            subsection_words = self.count_words(subsection)
            current_words = self.count_words(current_chunk)
            
            # Check if adding this subsection would exceed max size
            if current_words + subsection_words > self.max_chunk_size and current_chunk:
                # Save current chunk
                chunks.append(self._create_chunk(
                    chunk_id=chunk_id,
                    content=current_chunk,
                    source_file=source_file
                ))
                chunk_id += 1
                current_chunk = subsection
            else:
                # Add to current chunk
                if current_chunk:
                    current_chunk += "\n\n" + subsection
                else:
                    current_chunk = subsection
        
        # Save final chunk
        if current_chunk.strip():
            chunks.append(self._create_chunk(
                chunk_id=chunk_id,
                content=current_chunk,
                source_file=source_file
            ))
        
        return chunks
    
    def _create_chunk(self, chunk_id: int, content: str, source_file: str) -> Dict:
        """
        Create a chunk dictionary with metadata
        
        Args:
            chunk_id: Unique chunk identifier
            content: Chunk content
            source_file: Source filename
            
        Returns:
            dict: Chunk with metadata
        """
        
        word_count = self.count_words(content)
        section_type = self.identify_section_type(content)
        
        # Extract heading if present
        heading = ""
        heading_match = re.match(r'^(#{1,6}\s+.+)$', content, re.MULTILINE)
        if heading_match:
            heading = heading_match.group(1).strip()
        
        return {
            'chunk_id': chunk_id,
            'content': content,
            'word_count': word_count,
            'type': section_type,
            'source_file': os.path.basename(source_file),
            'heading': heading
        }
    
    def chunk_document(self, content: str, source_file: str) -> List[Dict]:
        """
        Main entry point for chunking a document
        
        Args:
            content: Document content
            source_file: Source filename
            
        Returns:
            list: List of chunk dictionaries
        """
        
        all_chunks = []
        chunk_id = 1
        
        # Split by major sections
        sections = self.split_by_headings(content)
        
        print(f"  â„¹ Split into {len(sections)} major sections")
        
        # Chunk each section
        for section in sections:
            section_chunks = self.chunk_section(
                section=section,
                source_file=source_file,
                chunk_id_start=chunk_id
            )
            
            all_chunks.extend(section_chunks)
            chunk_id += len(section_chunks)
        
        print(f"  âœ“ Created {len(all_chunks)} chunks")
        
        return all_chunks

print("âœ… SemanticChunker Part 3 (Main Logic) defined successfully!")

# ============================================
# COMPLETE DOCUMENT PROCESSING PIPELINE
# ============================================

class DocumentProcessor:
    """
    Complete pipeline for processing documents:
    1. Load documents from directory
    2. Preprocess content
    3. Chunk into semantic units
    4. Generate embeddings
    5. Store in vector database
    """
    
    def __init__(self, 
                 embeddings_model,
                 chunk_size: int = 500):
        """
        Initialize document processor
        
        Args:
            embeddings_model: Initialized embeddings model
            chunk_size: Target chunk size in words
        """
        self.embeddings_model = embeddings_model
        self.loader = DocumentLoader()
        self.preprocessor = DocumentPreprocessor()
        self.chunker = SemanticChunker(target_chunk_size=chunk_size)
        
        # Storage for processed data
        self.all_chunks = []
        self.all_metadata = []
    
    def process_directory(self, directory_path: str) -> List[Dict]:
        """
        Process all documents in a directory
        
        Args:
            directory_path: Path to directory containing documents
            
        Returns:
            list: List of all chunks from all documents
        """
        
        print(f"\n{'='*60}")
        print(f"ðŸ“ Processing directory: {directory_path}")
        print(f"{'='*60}\n")
        
        # Get all supported files
        path = Path(directory_path)
        supported_extensions = ['.pdf', '.docx', '.md', '.txt']
        
        all_files = []
        for ext in supported_extensions:
            all_files.extend(path.glob(f'*{ext}'))
        
        print(f"Found {len(all_files)} documents to process\n")
        
        # Process each file
        for file_path in tqdm(all_files, desc="Processing documents"):
            self._process_single_file(str(file_path))
        
        print(f"\n{'='*60}")
        print(f"âœ… Processing complete!")
        print(f"   Total documents: {len(all_files)}")
        print(f"   Total chunks: {len(self.all_chunks)}")
        print(f"{'='*60}\n")
        
        return self.all_chunks
    
    def _process_single_file(self, file_path: str):
        """
        Process a single document file
        
        Args:
            file_path: Path to document
        """
        
        print(f"\nðŸ“„ Processing: {os.path.basename(file_path)}")
        
        # Step 1: Load document
        content, file_type = self.loader.load_document(file_path)
        
        if not content:
            print(f"  âš  Skipping empty document")
            return
        
        # Step 2: Preprocess
        cleaned_content, metadata = self.preprocessor.preprocess(content)
        
        # Step 3: Chunk
        chunks = self.chunker.chunk_document(cleaned_content, file_path)
        
        # Step 4: Add to collection
        self.all_chunks.extend(chunks)
        
        # Store metadata
        self.all_metadata.append({
            'file_path': file_path,
            'file_type': file_type,
            'num_chunks': len(chunks),
            **metadata
        })
    
    def get_statistics(self) -> Dict:
        """
        Get processing statistics
        
        Returns:
            dict: Statistics about processed documents
        """
        
        if not self.all_chunks:
            return {}
        
        word_counts = [chunk['word_count'] for chunk in self.all_chunks]
        
        # Count chunks by type
        type_counts = {}
        for chunk in self.all_chunks:
            chunk_type = chunk['type']
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        stats = {
            'total_chunks': len(self.all_chunks),
            'total_documents': len(self.all_metadata),
            'avg_chunk_size': sum(word_counts) / len(word_counts),
            'min_chunk_size': min(word_counts),
            'max_chunk_size': max(word_counts),
            'chunks_by_type': type_counts
        }
        
        return stats

print("âœ… DocumentProcessor class defined successfully!")

# ============================================
# FAISS VECTOR DATABASE MANAGER
# ============================================

class FAISSVectorDB:
    """
    Manages FAISS vector database for similarity search
    """
    
    def __init__(self, embeddings_model):
        """
        Initialize FAISS vector database
        
        Args:
            embeddings_model: Initialized embeddings model
        """
        self.embeddings_model = embeddings_model
        self.index = None
        self.chunks = []
        self.dimension = None
    
    def create_embeddings(self, chunks: List[Dict]):
        """
        Create embeddings for all chunks and build FAISS index
        
        Args:
            chunks: List of chunk dictionaries
        """
        
        print(f"\n{'='*60}")
        print(f"ðŸ”¢ Creating embeddings for {len(chunks)} chunks...")
        print(f"{'='*60}\n")
        
        self.chunks = chunks
        
        # Extract text content from chunks
        texts = [chunk['content'] for chunk in chunks]
        
        # Generate embeddings in batches
        batch_size = 50  # Process 50 chunks at a time
        all_embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="Generating embeddings"):
            batch_texts = texts[i:i+batch_size]
            
            # Generate embeddings for batch
            batch_embeddings = self.embeddings_model.embed_documents(batch_texts)
            all_embeddings.extend(batch_embeddings)
        
        # Convert to numpy array
        embeddings_array = np.array(all_embeddings).astype('float32')
        
        # Get embedding dimension
        self.dimension = embeddings_array.shape,[object Object],
        
        print(f"\nâœ“ Generated embeddings: shape {embeddings_array.shape}")
        print(f"  Dimension: {self.dimension}")
        
        # Create FAISS index
        # IndexFlatL2 uses L2 (Euclidean) distance for similarity
        self.index = faiss.IndexFlatL2(self.dimension)
        
        # Add embeddings to index
        self.index.add(embeddings_array)
        
        print(f"âœ“ FAISS index created with {self.index.ntotal} vectors\n")
    
    def save_index(self, save_path: str = "./faiss_index"):
        """
        Save FAISS index and chunks to disk
        
        Args:
            save_path: Directory to save index
        """
        
        # Create directory if it doesn't exist
        os.makedirs(save_path, exist_ok=True)
        
        # Save FAISS index
        index_file = os.path.join(save_path, "index.faiss")
        faiss.write_index(self.index, index_file)
        
        # Save chunks as JSON
        chunks_file = os.path.join(save_path, "chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, indent=2, ensure_ascii=False)
        
        # Save metadata
        metadata_file = os.path.join(save_path, "metadata.json")
        metadata = {
            'dimension': self.dimension,
            'num_chunks': len(self.chunks),
            'index_type': 'IndexFlatL2'
        }
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"âœ… Index saved to: {save_path}")
        print(f"   - index.faiss ({os.path.getsize(index_file) / 1024:.2f} KB)")
        print(f"   - chunks.json ({os.path.getsize(chunks_file) / 1024:.2f} KB)")
    
    def load_index(self, load_path: str = "./faiss_index"):
        """
        Load FAISS index and chunks from disk
        
        Args:
            load_path: Directory containing saved index
        """
        
        # Load FAISS index
        index_file = os.path.join(load_path, "index.faiss")
        self.index = faiss.read_index(index_file)
        
        # Load chunks
        chunks_file = os.path.join(load_path, "chunks.json")
        with open(chunks_file, 'r', encoding='utf-8') as f:
            self.chunks = json.load(f)
        
        # Load metadata
        metadata_file = os.path.join(load_path, "metadata.json")
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
            self.dimension = metadata['dimension']
        
        print(f"âœ… Index loaded from: {load_path}")
        print(f"   Vectors: {self.index.ntotal}")
        print(f"   Chunks: {len(self.chunks)}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        Search for relevant chunks using semantic similarity
        
        Args:
            query: Search query
            top_k: Number of results to return
            
        Returns:
            list: List of relevant chunks with scores
        """
        
        # Generate embedding for query
        query_embedding = self.embeddings_model.embed_query(query)
        query_vector = np.array([query_embedding]).astype('float32')
        
        # Search in FAISS index
        # Returns distances and indices of nearest neighbors
        distances, indices = self.index.search(query_vector, top_k)
        
        # Prepare results
        results = []
        for idx, distance in zip(indices,[object Object],, distances,[object Object],):
            # Convert L2 distance to similarity score (0-1 range)
            # Lower distance = higher similarity
            similarity_score = 1 / (1 + distance)
            
            result = {
                'chunk': self.chunks[idx],
                'distance': float(distance),
                'similarity_score': float(similarity_score),
                'rank': len(results) + 1
            }
            results.append(result)
        
        return results

print("âœ… FAISSVectorDB class defined successfully!")

# ============================================
# RAG QUESTION ANSWERING SYSTEM
# ============================================

class RAGChatbot:
    """
    Retrieval-Augmented Generation chatbot
    Retrieves relevant context and generates answers
    """
    
    def __init__(self, llm, vector_db: FAISSVectorDB):
        """
        Initialize RAG chatbot
        
        Args:
            llm: Initialized language model
            vector_db: FAISS vector database
        """
        self.llm = llm
        self.vector_db = vector_db
    
    def create_prompt(self, query: str, context_chunks: List[Dict]) -> str:
        """
        Create prompt for LLM with retrieved context
        
        Args:
            query: User question
            context_chunks: Retrieved relevant chunks
            
        Returns:
            str: Formatted prompt
        """
        
        # Build context from retrieved chunks
        context_parts = []
        
        for i, chunk_data in enumerate(context_chunks, 1):
            chunk = chunk_data['chunk']
            score = chunk_data['similarity_score']
            
            context_parts.append(
                f"[Context {i}] (Relevance: {score:.2f})\n"
                f"Source: {chunk['source_file']}\n"
                f"Type: {chunk['type']}\n"
                f"Content:\n{chunk['content']}\n"
            )
        
        context_text = "\n" + "="*60 + "\n".join(context_parts)
        
        # Create prompt
        prompt = f"""You are an expert assistant specialized in Process Control Manuals (PCM) and compliance documentation.

Your task is to answer questions based ONLY on the provided context from the documents.

INSTRUCTIONS:
1. Answer the question using ONLY information from the provided context
2. If the context doesn't contain enough information, say so
3. Cite the source document when providing information
4. Be specific and accurate
5. If multiple sources provide relevant information, synthesize them
6. For procedures, list steps clearly
7. For tables, present information in structured format

CONTEXT FROM DOCUMENTS:
{context_text}

QUESTION: {query}

ANSWER (be detailed and cite sources):"""
        
        return prompt
    
    def answer_question(self, 
                       query: str, 
                       top_k: int = 5,
                       verbose: bool = True) -> Dict:
        """
        Answer a question using RAG
        
        Args:
            query: User question
            top_k: Number of context chunks to retrieve
            verbose: Whether to print detailed info
            
        Returns:
            dict: Answer with metadata
        """
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"â“ Question: {query}")
            print(f"{'='*60}\n")
        
        # Step 1: Retrieve relevant chunks
        if verbose:
            print("ðŸ” Retrieving relevant context...")
        
        retrieved_chunks = self.vector_db.search(query, top_k=top_k)
        
        if verbose:
            print(f"âœ“ Retrieved {len(retrieved_chunks)} relevant chunks\n")
            
            print("ðŸ“„ Retrieved Sources:")
            for i, chunk_data in enumerate(retrieved_chunks, 1):
                chunk = chunk_data['chunk']
                score = chunk_data['similarity_score']
                print(f"   {i}. {chunk['source_file']} "
                      f"(Type: {chunk['type']}, "
                      f"Similarity: {score:.3f})")
            print()
        
        # Step 2: Create prompt with context
        prompt = self.create_prompt(query, retrieved_chunks)
        
        # Step 3: Generate answer
        if verbose:
            print("ðŸ¤– Generating answer...\n")
        
        try:
            # Generate response from LLM
            answer = self.llm.invoke(prompt)
            
            # Extract text from response
            if hasattr(answer, 'content'):
                answer_text = answer.content
            elif hasattr(answer, 'text'):
                answer_text = answer.text
            else:
                answer_text = str(answer)
            
        except Exception as e:
            answer_text = f"Error generating answer: {str(e)}"
        
        # Prepare result
        result = {
            'question': query,
            'answer': answer_text,
            'sources': [
                {
                    'file': chunk_data['chunk']['source_file'],
                    'type': chunk_data['chunk']['type'],
                    'similarity': chunk_data['similarity_score']
                }
                for chunk_data in retrieved_chunks
            ],
            'num_sources': len(retrieved_chunks)
        }
        
        if verbose:
            print(f"{'='*60}")
            print(f"ðŸ’¬ ANSWER:")
            print(f"{'='*60}")
            print(answer_text)
            print(f"\n{'='*60}\n")
        
        return result
    
    def interactive_session(self):
        """
        Start an interactive Q&A session
        """
        
        print(f"\n{'='*60}")
        print(f"ðŸ¤– RAG Chatbot - Interactive Session")
        print(f"{'='*60}")
        print(f"Type your questions (or 'quit' to exit)\n")
        
        while True:
            query = input("â“ Your question: ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("\nðŸ‘‹ Goodbye!")
                break
            
            if not query:
                continue
            
            self.answer_question(query, verbose=True)

print("âœ… RAGChatbot class defined successfully!")

# ============================================
# MAIN EXECUTION - PROCESS DOCUMENTS
# ============================================

# Set your documents directory path
DOCUMENTS_DIR = "./Doc"  # Change this to your directory path

# Check if directory exists
if not os.path.exists(DOCUMENTS_DIR):
    print(f"âŒ Directory not found: {DOCUMENTS_DIR}")
    print(f"Please create the directory and add your documents")
else:
    print(f"âœ… Documents directory found: {DOCUMENTS_DIR}\n")
    
    # Initialize document processor
    processor = DocumentProcessor(
        embeddings_model=embeddings,
        chunk_size=500  # Target 500 words per chunk
    )
    
    # Process all documents in directory
    all_chunks = processor.process_directory(DOCUMENTS_DIR)
    
    # Show statistics
    stats = processor.get_statistics()
    
    print(f"\n{'='*60}")
    print(f"ðŸ“Š PROCESSING STATISTICS")
    print(f"{'='*60}")
    print(f"Total Documents: {stats['total_documents']}")
    print(f"Total Chunks: {stats['total_chunks']}")
    print(f"Average Chunk Size: {stats['avg_chunk_size']:.1f} words")
    print(f"Min Chunk Size: {stats['min_chunk_size']} words")
    print(f"Max Chunk Size: {stats['max_chunk_size']} words")
    print(f"\nChunks by Type:")
    for chunk_type, count in stats['chunks_by_type'].items():
        print(f"  - {chunk_type}: {count}")
    print(f"{'='*60}\n")

# ============================================
# CREATE AND SAVE FAISS INDEX
# ============================================

# Initialize FAISS vector database
vector_db = FAISSVectorDB(embeddings_model=embeddings)

# Create embeddings and build index
vector_db.create_embeddings(all_chunks)

# Save index to disk
FAISS_INDEX_PATH = "./faiss_index"
vector_db.save_index(FAISS_INDEX_PATH)

print(f"\nâœ… Vector database ready for querying!")


# ============================================
# TEST RETRIEVAL
# ============================================

# Test search functionality
test_query = "What is the access request process?"

print(f"\n{'='*60}")
print(f"ðŸ” Testing retrieval with query:")
print(f"   '{test_query}'")
print(f"{'='*60}\n")

results = vector_db.search(test_query, top_k=3)

print(f"Retrieved {len(results)} results:\n")

for i, result in enumerate(results, 1):
    chunk = result['chunk']
    print(f"{i}. Source: {chunk['source_file']}")
    print(f"   Type: {chunk['type']}")
    print(f"   Similarity: {result['similarity_score']:.3f}")
    print(f"   Preview: {chunk['content'][:200]}...")
    print(f"   {'-'*50}\n")


# ============================================
# INITIALIZE RAG CHATBOT
# ============================================

# Create RAG chatbot
chatbot = RAGChatbot(llm=llm, vector_db=vector_db)

print("âœ… RAG Chatbot initialized and ready!")
print("\nYou can now ask questions using:")
print("   chatbot.answer_question('your question here')")
print("\nOr start interactive session:")
print("   chatbot.interactive_session()")

# ============================================
# ASK QUESTIONS
# ============================================

# Example questions
questions = [
    "What is the document owner's name?",
    "How do I submit an access request?",
    "What is the escalation process?",
    "Where is evidence stored?",
]

# Ask each question
for question in questions:
    result = chatbot.answer_question(question, top_k=3, verbose=True)
    print("\n" + "="*60 + "\n")

# ============================================
# INTERACTIVE SESSION
# ============================================

# Start interactive Q&A session
# Type your questions and get answers
# Type 'quit' to exit

chatbot.interactive_session()

# ============================================
# LOAD EXISTING INDEX (SKIP PROCESSING)
# ============================================

# Use this cell in future sessions to load existing index
# without reprocessing all documents

# Initialize new vector database
vector_db_loaded = FAISSVectorDB(embeddings_model=embeddings)

# Load saved index
vector_db_loaded.load_index("./faiss_index")

# Create chatbot with loaded index
chatbot_loaded = RAGChatbot(llm=llm, vector_db=vector_db_loaded)

print("âœ… Loaded existing index - ready to answer questions!")

# ============================================
# INSPECT SAMPLE CHUNKS
# ============================================

# View some sample chunks to verify quality

print(f"\n{'='*60}")
print(f"ðŸ“„ SAMPLE CHUNKS")
print(f"{'='*60}\n")

# Show first 3 chunks
for i in range(min(3, len(all_chunks))):
    chunk = all_chunks[i]
    
    print(f"Chunk {chunk['chunk_id']}:")
    print(f"  Source: {chunk['source_file']}")
    print(f"  Type: {chunk['type']}")
    print(f"  Words: {chunk['word_count']}")
    print(f"  Heading: {chunk.get('heading', 'N/A')}")
    print(f"\n  Content Preview:")
    print(f"  {'-'*56}")
    print(f"  {chunk['content'][:300]}...")
    print(f"  {'-'*56}\n")

# ============================================
# UTILITY FUNCTIONS
# ============================================

def search_by_source(source_file: str):
    """Find all chunks from a specific source file"""
    matching_chunks = [
        chunk for chunk in all_chunks 
        if chunk['source_file'] == source_file
    ]
    
    print(f"\nðŸ“„ Chunks from {source_file}:")
    print(f"   Total: {len(matching_chunks)} chunks\n")
    
    for chunk in matching_chunks[:5]:  # Show first 5
        print(f"   Chunk {chunk['chunk_id']}: {chunk['type']} "
              f"({chunk['word_count']} words)")
    
    if len(matching_chunks) > 5:
        print(f"   ... and {len(matching_chunks) - 5} more")

def search_by_type(chunk_type: str):
    """Find all chunks of a specific type"""
    matching_chunks = [
        chunk for chunk in all_chunks 
        if chunk['type'] == chunk_type
    ]
    
    print(f"\nðŸ“‹ Chunks of type '{chunk_type}':")
    print(f"   Total: {len(matching_chunks)} chunks\n")
    
    for chunk in matching_chunks[:5]:  # Show first 5
        print(f"   {chunk['source_file']}: {chunk['word_count']} words")
    
    if len(matching_chunks) > 5:
        print(f"   ... and {len(matching_chunks) - 5} more")

def export_chunks_to_json(output_file: str = "chunks_export.json"):
    """Export all chunks to JSON file"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_chunks, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Exported {len(all_chunks)} chunks to {output_file}")

print("âœ… Utility functions defined!")
print("\nAvailable functions:")
print("  - search_by_source('filename.md')")
print("  - search_by_type('table')")
print("  - export_chunks_to_json('output.json')")




